{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Pk2GDiu4Bb"
      },
      "source": [
        "# Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EyOhCiEu4Bk",
        "outputId": "e760e61d-2248-4833-d173-31d4b656e0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from snntorch) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->snntorch) (4.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->snntorch) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->snntorch) (2022.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (4.1.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.10.0)\n",
            "mkdir: cannot create directory ‘tmp’: File exists\n",
            "mkdir: cannot create directory ‘logs’: File exists\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "mode = -1\n",
        "try:\n",
        "    os.getlogin() # This will return an exception if ran in Google colab\n",
        "    mode = 'local'\n",
        "except:\n",
        "    # Install necessary files and create necessary folders (Only when ran in Google Drive)\n",
        "    !pip install snntorch\n",
        "    !pip install torch\n",
        "    !pip install gym\n",
        "    !pip install gym[box2d]\n",
        "    !mkdir tmp\n",
        "    !mkdir logs\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive')\n",
        "    #!cp \"/content/drive/MyDrive/Colab Notebooks/dict9 LIF\" \"/content/tmp/\"\n",
        "    mode = 'colab'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv0oAX5su4Bp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "YaOc25LDu4Br"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "from datetime import datetime\n",
        "from collections import deque, namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import surrogate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1W8CBfWbvZM8"
      },
      "outputs": [],
      "source": [
        "# A small method to save plots\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtnGruUvu4Bv"
      },
      "source": [
        "# Parameters ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uSJzX05Vu4B1"
      },
      "outputs": [],
      "source": [
        "lr = 5e-4               # Learning rate\n",
        "epsilon = 0.3\n",
        "epsilon_end = 0.001\n",
        "epsilon_decay = 0.995\n",
        "gamma = 0.99\n",
        "batch_size = 128\n",
        "mean_every = 2\n",
        "update_every = 5\n",
        "tau = 1e-3\n",
        "gauss_per_dim = 64\n",
        "n_games = 1000          # Number of games to train for\n",
        "n_rounds = 5            # Number of competitive rounds to play"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rAqS6F-Yu4By",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8b2487-c507-45b3-9b8e-e3e83fb31047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Env name (clean): bipedalwalker_v3\n",
            "Datetime (clean): 2022_11_22_10_31_37\n",
            "Output file: bipedalwalker_v3scoreboard.txt\n",
            "Model file: ./tmp/bipedalwalker_v3_dict9\n",
            "Running training on:  cpu\n",
            "Environment: colab\n"
          ]
        }
      ],
      "source": [
        "env_name = 'BipedalWalker-v3'\n",
        "env_name_clean = env_name.lower().replace('-','_')\n",
        "filename = env_name_clean + \".png\"\n",
        "outfilename = env_name_clean + \"scoreboard.txt\"\n",
        "datename = str(datetime.now()).split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_') \n",
        "scorefigname = env_name_clean + '_' + datename + '_scores.png'\n",
        "epsfigname = env_name_clean + '_' + datename + '_eps_history.png'\n",
        "cumfigname = env_name_clean + '_' + datename + '_cumulated_reward.png'\n",
        "distfigname = env_name_clean + '_' + datename + '_distances.png'\n",
        "meandistfigname = env_name_clean + '_' + datename + '_mean_distances.png'\n",
        "model_file = './tmp/'  + env_name_clean + '_dict9'\n",
        "model_file_target = model_file + '_target'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Env name (clean):',env_name_clean)\n",
        "print('Datetime (clean):', datename)\n",
        "print('Output file:', outfilename)\n",
        "print('Model file:', model_file)\n",
        "print('Running training on: ', device)\n",
        "print('Environment:', mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU6d2CLFu4B3"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Net"
      ],
      "metadata": {
        "id": "dlbfSeXDSIiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jdQHnqisgu4v"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, beta=0.95):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Initialize grad objects        \n",
        "        spike_grad1 = surrogate.fast_sigmoid()\n",
        "        spike_grad2 = surrogate.fast_sigmoid()\n",
        "        spike_grad3 = surrogate.fast_sigmoid()\n",
        "        spike_grad4 = surrogate.fast_sigmoid()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, 128)\n",
        "        self.lif1 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad1)\n",
        "        \n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.lif2 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad2)\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.lif3 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad3)\n",
        "\n",
        "        self.fc4 = nn.Linear(32, num_outputs)\n",
        "        self.lif4 = snn.Leaky(beta=beta, learn_beta=True, threshold=1e5, reset_mechanism=\"none\", spike_grad=spike_grad4)\n",
        "\n",
        "    def _convert_to_spikes(self, data):\n",
        "        return snn.spikegen.delta(data, threshold=0.1, padding=False, off_spike=True)\n",
        "\n",
        "    def createGauss(self, mins, maxes, numPerDim, amplMax, dims):\n",
        "        self.amplMax = amplMax\n",
        "        self.numPerDim = numPerDim\n",
        "        self.M = []\n",
        "        self.sigma = []\n",
        "        for i in range(dims):\n",
        "            M, sigma = np.linspace(mins[i], maxes[i], numPerDim, retstep=True)\n",
        "            self.M.append(M)\n",
        "            self.sigma += [sigma, ] * self.numPerDim\n",
        "        self.M = torch.tensor(np.array(self.M).reshape(-1, self.numPerDim), dtype=torch.float).to(device)\n",
        "        self.sigma = torch.tensor(np.array(self.sigma).reshape(-1, self.numPerDim), dtype=torch.float).to(device)\n",
        "\n",
        "    def gaussianCurrents(self, data):\n",
        "        x = data.unsqueeze(-1).repeat([1, 1, self.numPerDim])\n",
        "        return (torch.exp(-1 / 2 * ((x - self.M) / self.sigma) ** 2) * self.amplMax).reshape(data.shape[0], -1)\n",
        "\n",
        "    def forward(self, x, num_steps=16):\n",
        "        x = self.gaussianCurrents(x)\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        # Record the final layer#\n",
        "        spk1_rec = []\n",
        "        mem1_rec = []\n",
        "        \n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "        \n",
        "        spk3_rec = []\n",
        "        mem3_rec = []        \n",
        "\n",
        "        spk4_rec = []\n",
        "        mem4_rec = []  \n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            \n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            \n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            cur4 = self.fc4(spk3)\n",
        "            spk4, mem4 = self.lif4(cur4, mem4)\n",
        "\n",
        "            spk1_rec.append(spk1)\n",
        "            mem1_rec.append(mem1)\n",
        "            \n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "            \n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "            \n",
        "            spk4_rec.append(spk3)\n",
        "            mem4_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk4_rec, dim=0), torch.stack(mem4_rec, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReplayBuffer"
      ],
      "metadata": {
        "id": "eywnQwfISDqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed=42):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "izTUGxP5QsoG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "W4KED1jNSLIR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-QRt0zUKgmQB"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, state_size, action_size, seed, buffer_size, batch_size, gamma, tau, learning_rate, update_every, low, high, gaussPerDim, model_file):\n",
        "        # Params\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.state_size = state_size\n",
        "        self.n_actions = 3 ** action_size\n",
        "        self.tau = tau\n",
        "        self.update_every = update_every\n",
        "        self.seed = random.seed(seed)\n",
        "        self.model_file = model_file\n",
        "        self.low = low\n",
        "        self.high = high\n",
        "        self.dtype = torch.float\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.mem_size = 1000000\n",
        "        self.gaussPerDim = gaussPerDim\n",
        "\n",
        "        num_inputs = state_size * gaussPerDim\n",
        "        num_hidden = -1 # This param is not used currently\n",
        "        num_outputs = action_size\n",
        "\n",
        "        # Q-Network\n",
        "        ## Local\n",
        "        self.qnetwork_local = Net(num_inputs=num_inputs, num_hidden=num_hidden, num_outputs=num_outputs).to(device)\n",
        "        self.qnetwork_local.createGauss(self.low, self.high, gaussPerDim, 1.0, state_size)\n",
        "        \n",
        "        ## Target\n",
        "        self.qnetwork_target = Net(num_inputs=num_inputs, num_hidden=num_hidden, num_outputs=num_outputs).to(device)\n",
        "        self.qnetwork_target.createGauss(self.low, self.high, gaussPerDim, 1.0, state_size)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.qnetwork_local.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
        "        \n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def get_action(self, action_idx):\n",
        "        action = []\n",
        "        \n",
        "        # Joint 1\n",
        "        output = int(action_idx / 27) - 1\n",
        "        rest = action_idx - 27 * int(action_idx / 27)\n",
        "        action.append(output)\n",
        "        \n",
        "        # Joint 2\n",
        "        output = int(rest / 9) - 1\n",
        "        rest = rest - 9 * int(rest / 9)\n",
        "        action.append(output)\n",
        "        \n",
        "        # Joint 3\n",
        "        output = int(rest / 3) - 1\n",
        "        rest = rest - 3 * int(rest / 3)\n",
        "        action.append(output)\n",
        "        \n",
        "        # Joint 4\n",
        "        action.append(rest - 1)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def act(self, state, eps): # Choose action based on state\n",
        "        state = state[np.newaxis, :]\n",
        "        with torch.no_grad():\n",
        "            self.qnetwork_local.eval()\n",
        "            spikes, actions = self.qnetwork_local(torch.tensor(state, dtype=self.dtype).to(self.device))\n",
        "            actions = actions.cpu()[-1, 0, :].detach().numpy()\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            action_idx = np.argmax(actions)\n",
        "        else:\n",
        "            action_idx = int(random.randrange(self.n_actions))\n",
        "        return self.get_action(action_idx)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            \n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.batch_size:\n",
        "                experiences = self.memory.sample()\n",
        "                return self.learn(experiences, self.gamma)\n",
        "        return None\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Get max predicted Q values (for next states) from target model\n",
        "        spikes, Q_targets_next = self.qnetwork_target(next_states)\n",
        "        \n",
        "        # Compute Q targets for current states \n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        # Get expected Q values from local model\n",
        "        spikes, Q_expected = self.qnetwork_local(states)#.gather(1, actions)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        \n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
        "        return loss.cpu().data.numpy()\n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        # Update the target model parameters based on tau (soft update learning rate)\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def save_models(self):\n",
        "        torch.save(self.qnetwork_local.state_dict(), self.model_file)\n",
        "    \n",
        "    def load_models(self):\n",
        "        self.qnetwork_local.load_state_dict(torch.load(self.model_file, map_location=torch.device('cpu')))\n",
        "        self.qnetwork_local.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEA3d-s3u4B8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "2kHOTP8yu4B8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61389c3-d23b-42b7-d616-037ab313c519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gymtype: box\n",
            "input dims: 24\n",
            "action size: 4\n",
            "lif1 beta: tensor(0.9500)\n",
            "lif2 beta: tensor(0.9500)\n",
            "lif3 beta: tensor(1.)\n",
            "Action space: Box(-1.0, 1.0, (4,), float32)\n",
            "Observation space: Box([-3.1415927 -5.        -5.        -5.        -3.1415927 -5.\n",
            " -3.1415927 -5.        -0.        -3.1415927 -5.        -3.1415927\n",
            " -5.        -0.        -1.        -1.        -1.        -1.\n",
            " -1.        -1.        -1.        -1.        -1.        -1.       ], [3.1415927 5.        5.        5.        3.1415927 5.        3.1415927\n",
            " 5.        5.        3.1415927 5.        3.1415927 5.        5.\n",
            " 1.        1.        1.        1.        1.        1.        1.\n",
            " 1.        1.        1.       ], (24,), float32)\n"
          ]
        }
      ],
      "source": [
        "stime = time.time()\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "\n",
        "n_actions = -1\n",
        "gymtype = -1\n",
        "if(type(env.action_space) == gym.spaces.box.Box):    \n",
        "    n_actions = int(env.action_space.shape[0])\n",
        "    gymtype = 'box'\n",
        "else:\n",
        "    n_actions = int(env.action_space.n)\n",
        "    gymtype = 'disc'\n",
        "\n",
        "n_input_dims = int(env.observation_space.shape[0]) # If the state space is not an 1-D vector then we need to change this\n",
        "\n",
        "agent = Agent(state_size=n_input_dims, \n",
        "              action_size=n_actions, \n",
        "              seed=420, \n",
        "              buffer_size=int(1e5), \n",
        "              batch_size=batch_size, \n",
        "              gamma=gamma, \n",
        "              tau=tau, \n",
        "              learning_rate=lr, \n",
        "              update_every=update_every, \n",
        "              low=env.observation_space.low, \n",
        "              high=env.observation_space.high, \n",
        "              gaussPerDim = gauss_per_dim, \n",
        "              model_file=model_file)\n",
        "\n",
        "scores = [] # Logged every episode\n",
        "eps_history = [] # Logged every episode\n",
        "avg_scores = [] # Logged every 10 episodes\n",
        "\n",
        "if(os.path.exists(model_file)):\n",
        "    try: \n",
        "        agent.load_models()\n",
        "    except:\n",
        "        !rm -rf model_file\n",
        "        agent.save_models()\n",
        "else:\n",
        "    agent.save_models()\n",
        "\n",
        "state = agent.qnetwork_local.state_dict()\n",
        "state[\"lif1.beta\"] = torch.tensor(0.95,dtype=torch.float).to(device)\n",
        "state[\"lif2.beta\"] = torch.tensor(0.95, dtype=torch.float).to(device)\n",
        "state[\"lif3.beta\"] = torch.tensor(1.0, dtype=torch.float).to(device)\n",
        "# state[\"lif4.beta\"] = torch.tensor(0.95, dtype=torch.float).to(device)\n",
        "# state[\"lif5.beta\"] = torch.tensor(1.0, dtype=torch.float).to(device)\n",
        "agent.qnetwork_local.load_state_dict(state)\n",
        "\n",
        "print('gymtype:', gymtype)\n",
        "print('input dims:', n_input_dims)\n",
        "print('action size:', n_actions)\n",
        "print('lif1 beta:', agent.qnetwork_local.lif1.beta)\n",
        "print('lif2 beta:', agent.qnetwork_local.lif2.beta)\n",
        "print('lif3 beta:', agent.qnetwork_local.lif3.beta)\n",
        "print('Action space:', env.action_space)\n",
        "print('Observation space:', env.observation_space)\n",
        "#print('High:', env.observation_space.high)\n",
        "#print('Low:', env.observation_space.low)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu7TQfmXu4CA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "mean_scores = []\n",
        "last_scores = deque(maxlen=mean_every)\n",
        "distances = []\n",
        "mean_distances = []\n",
        "last_distance = deque(maxlen=mean_every)\n",
        "losses_mean_episode = []\n",
        "eps_history = []\n",
        "\n",
        "ep = -1\n",
        "while ep < n_games:\n",
        "    ep += 1\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    total_distance = 0\n",
        "    losses = []\n",
        "    \n",
        "    while(not done):\n",
        "        # Choose action\n",
        "        action = agent.act(state, epsilon)\n",
        "        \n",
        "        # Execute action and observe environment\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        # Calculate loss and learn\n",
        "        loss = agent.step(state, action, reward, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        total_reward += reward\n",
        "\n",
        "        if(loss is not None):\n",
        "            losses.append(loss)\n",
        "\n",
        "        if(reward != -100):\n",
        "            total_distance += reward\n",
        "    \n",
        "    if epsilon > epsilon_end:\n",
        "        epsilon = epsilon * epsilon_decay\n",
        "\n",
        "    if len(losses) >= 1:\n",
        "        mean_loss = np.mean(losses)\n",
        "        losses_mean_episode.append((ep, mean_loss))\n",
        "    else:\n",
        "        mean_loss = None\n",
        "\n",
        "    # print('episode:',ep,'/',n_games,'\\t score:',total_reward,'\\t distance:',total_distance,'\\t loss:',mean_loss,'\\t epsilon:',epsilon)\n",
        "\n",
        "    scores.append(total_reward)\n",
        "    distances.append(total_distance)\n",
        "    last_scores.append(total_reward)\n",
        "    last_distance.append(total_distance)\n",
        "    mean_score = np.mean(last_scores)\n",
        "    mean_distance = np.mean(last_distance)\n",
        "    eps_history.append(epsilon)\n",
        "\n",
        "    # Record rewards dynamically\n",
        "    FILE = 'record.dat'\n",
        "    data = [ep, total_reward, total_distance, mean_loss, epsilon]\n",
        "    with open(FILE, \"ab\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    if (mean_score >= 300):\n",
        "        print('environment solved in:',ep,'\\t mean score:',mean_score)\n",
        "        torch.save(agent.qnetwork_local.state_dict(), model_file)\n",
        "        torch.save(agent.qnetwork_target.state_dict(), model_file_target)\n",
        "        break\n",
        "\n",
        "    # Save model every mean_every episodes\n",
        "    if ((ep % mean_every) == 0):        \n",
        "        print('episode:',ep,'/',n_games,'\\t mean score:',mean_score,'\\t mean distance:',mean_distance,'\\t mean loss:',mean_loss,'\\t epsilon:',epsilon)\n",
        "\n",
        "        torch.save(agent.qnetwork_local.state_dict(), model_file)\n",
        "        torch.save(agent.qnetwork_target.state_dict(), model_file_target)\n",
        "        \n",
        "        mean_scores.append(mean_score)\n",
        "        mean_distances.append(mean_distance)\n",
        "        \n",
        "        FILE = 'record_mean.dat'\n",
        "        data = [ep, mean_score, mean_distance, mean_loss, epsilon]\n",
        "        with open(FILE, \"ab\") as f:\n",
        "            pickle.dump(data, f)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "kqnqogd1cV8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3737c013-3951-4079-e96f-1c2d037b13e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0 / 1000 \t mean score: -125.43579899252951 \t mean distance: -25.43579899252951 \t mean loss: None \t epsilon: 0.2985\n",
            "episode: 2 / 1000 \t mean score: -103.84364219289087 \t mean distance: -3.8436421928908704 \t mean loss: 155.99957 \t epsilon: 0.29552246249999997\n",
            "episode: 4 / 1000 \t mean score: -101.76145420882943 \t mean distance: -1.7614542088294367 \t mean loss: 134.7056 \t epsilon: 0.29257462593656247\n",
            "episode: 6 / 1000 \t mean score: -104.62154248938378 \t mean distance: -54.62154248938378 \t mean loss: 32.084675 \t epsilon: 0.28965619404284526\n",
            "episode: 8 / 1000 \t mean score: -112.0750883904056 \t mean distance: -12.075088390405599 \t mean loss: 6.9733815 \t epsilon: 0.28676687350726787\n",
            "episode: 10 / 1000 \t mean score: -120.36160414412679 \t mean distance: -20.361604144126787 \t mean loss: 24.497347 \t epsilon: 0.28390637394403284\n",
            "episode: 12 / 1000 \t mean score: -108.76467816660367 \t mean distance: -8.764678166603671 \t mean loss: 21.594923 \t epsilon: 0.2810744078639411\n",
            "episode: 14 / 1000 \t mean score: -110.75316792628013 \t mean distance: -10.75316792628013 \t mean loss: 26.046385 \t epsilon: 0.27827069064549825\n",
            "episode: 16 / 1000 \t mean score: -100.01604586703237 \t mean distance: -0.016045867032371375 \t mean loss: 24.387495 \t epsilon: 0.2754949405063094\n",
            "episode: 18 / 1000 \t mean score: -83.4859643783098 \t mean distance: -33.485964378309795 \t mean loss: 18.845509 \t epsilon: 0.272746878474759\n",
            "episode: 20 / 1000 \t mean score: -103.33867743474123 \t mean distance: -53.338677434741236 \t mean loss: 10.496869 \t epsilon: 0.27002622836197326\n",
            "episode: 22 / 1000 \t mean score: -117.51282780088988 \t mean distance: -17.51282780088989 \t mean loss: 7.753534 \t epsilon: 0.26733271673406256\n",
            "episode: 24 / 1000 \t mean score: -89.44085772146292 \t mean distance: -39.44085772146291 \t mean loss: 8.259753 \t epsilon: 0.2646660728846403\n",
            "episode: 26 / 1000 \t mean score: -101.20643205596414 \t mean distance: -1.2064320559641364 \t mean loss: 9.819971 \t epsilon: 0.26202602880761594\n",
            "episode: 28 / 1000 \t mean score: -104.53713542793687 \t mean distance: -4.5371354279368825 \t mean loss: 8.967809 \t epsilon: 0.25941231917026\n",
            "episode: 30 / 1000 \t mean score: -98.88054996835751 \t mean distance: 1.1194500316424878 \t mean loss: 20.746124 \t epsilon: 0.2568246812865366\n",
            "episode: 32 / 1000 \t mean score: -109.95334864748705 \t mean distance: -9.953348647487033 \t mean loss: 6.2641873 \t epsilon: 0.2542628550907034\n",
            "episode: 34 / 1000 \t mean score: -110.3039324724426 \t mean distance: -10.303932472442586 \t mean loss: 11.861122 \t epsilon: 0.25172658311117363\n",
            "episode: 36 / 1000 \t mean score: -108.45291931188012 \t mean distance: -8.45291931188012 \t mean loss: 8.448954 \t epsilon: 0.24921561044463966\n",
            "episode: 38 / 1000 \t mean score: -111.17740751222146 \t mean distance: -11.17740751222147 \t mean loss: 5.7311907 \t epsilon: 0.24672968473045437\n",
            "episode: 40 / 1000 \t mean score: -101.18899244116679 \t mean distance: -1.1889924411667958 \t mean loss: 8.059794 \t epsilon: 0.24426855612526807\n",
            "episode: 42 / 1000 \t mean score: -101.51629214846466 \t mean distance: -1.5162921484646608 \t mean loss: 10.815371 \t epsilon: 0.24183197727791853\n",
            "episode: 44 / 1000 \t mean score: -113.83352736445018 \t mean distance: -13.833527364450179 \t mean loss: 7.52081 \t epsilon: 0.2394197033045713\n",
            "episode: 46 / 1000 \t mean score: -113.73738552143269 \t mean distance: -13.737385521432689 \t mean loss: 8.728933 \t epsilon: 0.2370314917641082\n",
            "episode: 48 / 1000 \t mean score: -114.99915127974623 \t mean distance: -14.99915127974622 \t mean loss: 16.763401 \t epsilon: 0.23466710263376123\n",
            "episode: 50 / 1000 \t mean score: -111.71839931258899 \t mean distance: -11.718399312588996 \t mean loss: 18.093431 \t epsilon: 0.23232629828498946\n",
            "episode: 52 / 1000 \t mean score: -119.94642498509802 \t mean distance: -19.946424985098016 \t mean loss: 13.269925 \t epsilon: 0.23000884345959668\n",
            "episode: 54 / 1000 \t mean score: -114.15821948660101 \t mean distance: -14.15821948660101 \t mean loss: 13.567193 \t epsilon: 0.22771450524608722\n",
            "episode: 56 / 1000 \t mean score: -112.84364965052404 \t mean distance: -12.843649650524043 \t mean loss: 7.990419 \t epsilon: 0.2254430530562575\n",
            "episode: 58 / 1000 \t mean score: -111.70143652581702 \t mean distance: -11.701436525817012 \t mean loss: 11.794954 \t epsilon: 0.22319425860202133\n",
            "episode: 60 / 1000 \t mean score: -118.77102289004993 \t mean distance: -18.771022890049935 \t mean loss: 14.395349 \t epsilon: 0.22096789587246615\n",
            "episode: 62 / 1000 \t mean score: -103.44041796859943 \t mean distance: -3.440417968599423 \t mean loss: 11.634083 \t epsilon: 0.2187637411111383\n",
            "episode: 64 / 1000 \t mean score: -111.12471755157813 \t mean distance: -11.124717551578133 \t mean loss: 11.279537 \t epsilon: 0.2165815727935547\n",
            "episode: 66 / 1000 \t mean score: -110.34626193339346 \t mean distance: -10.346261933393457 \t mean loss: 13.85799 \t epsilon: 0.21442117160493898\n",
            "episode: 68 / 1000 \t mean score: -125.92683817924495 \t mean distance: -25.92683817924495 \t mean loss: 10.083823 \t epsilon: 0.2122823204181797\n",
            "episode: 70 / 1000 \t mean score: -114.23696367499974 \t mean distance: -14.236963674999746 \t mean loss: 16.995314 \t epsilon: 0.21016480427200837\n",
            "episode: 72 / 1000 \t mean score: -107.40707040419085 \t mean distance: -7.407070404190838 \t mean loss: 14.474593 \t epsilon: 0.2080684103493951\n",
            "episode: 74 / 1000 \t mean score: -122.6411093137469 \t mean distance: -22.641109313746895 \t mean loss: 11.981458 \t epsilon: 0.2059929279561599\n",
            "episode: 76 / 1000 \t mean score: -112.68972096361031 \t mean distance: -12.689720963610325 \t mean loss: 16.150803 \t epsilon: 0.2039381484997972\n",
            "episode: 78 / 1000 \t mean score: -102.26207036697178 \t mean distance: -2.2620703669717925 \t mean loss: 16.382847 \t epsilon: 0.2019038654685117\n",
            "episode: 80 / 1000 \t mean score: -122.86949175737641 \t mean distance: -22.86949175737641 \t mean loss: 19.76576 \t epsilon: 0.1998898744104633\n",
            "episode: 82 / 1000 \t mean score: -122.793899199261 \t mean distance: -22.793899199261006 \t mean loss: 14.519902 \t epsilon: 0.19789597291321892\n",
            "episode: 84 / 1000 \t mean score: -111.08579700239932 \t mean distance: -11.085797002399325 \t mean loss: 26.822525 \t epsilon: 0.19592196058340955\n",
            "episode: 86 / 1000 \t mean score: -111.35103095954096 \t mean distance: -11.35103095954096 \t mean loss: 27.244465 \t epsilon: 0.19396763902659003\n",
            "episode: 88 / 1000 \t mean score: -113.79422898206795 \t mean distance: -13.794228982067953 \t mean loss: 11.03847 \t epsilon: 0.1920328118272998\n",
            "episode: 90 / 1000 \t mean score: -121.84334399433618 \t mean distance: -21.843343994336188 \t mean loss: 19.67517 \t epsilon: 0.1901172845293225\n",
            "episode: 92 / 1000 \t mean score: -109.20042693011897 \t mean distance: -9.200426930118965 \t mean loss: 30.276834 \t epsilon: 0.1882208646161425\n",
            "episode: 94 / 1000 \t mean score: -122.51380740318695 \t mean distance: -22.513807403186934 \t mean loss: 24.164684 \t epsilon: 0.18634336149159647\n",
            "episode: 96 / 1000 \t mean score: -123.24242637928945 \t mean distance: -23.242426379289455 \t mean loss: 9.310347 \t epsilon: 0.18448458646071778\n",
            "episode: 98 / 1000 \t mean score: -118.33533350824254 \t mean distance: -18.335333508242535 \t mean loss: 17.212708 \t epsilon: 0.1826443527107721\n",
            "episode: 100 / 1000 \t mean score: -113.54362551065003 \t mean distance: -13.543625510650035 \t mean loss: 10.706769 \t epsilon: 0.18082247529248216\n",
            "episode: 102 / 1000 \t mean score: -120.18793373256673 \t mean distance: -20.187933732566723 \t mean loss: 13.44598 \t epsilon: 0.17901877110143966\n",
            "episode: 104 / 1000 \t mean score: -115.0309612573742 \t mean distance: -15.030961257374202 \t mean loss: 20.359112 \t epsilon: 0.17723305885970278\n",
            "episode: 106 / 1000 \t mean score: -116.4159416356965 \t mean distance: -16.41594163569649 \t mean loss: 16.960941 \t epsilon: 0.17546515909757723\n",
            "episode: 108 / 1000 \t mean score: -123.37203908963872 \t mean distance: -23.37203908963872 \t mean loss: 16.002926 \t epsilon: 0.1737148941355789\n",
            "episode: 110 / 1000 \t mean score: -123.40787828597239 \t mean distance: -23.40787828597238 \t mean loss: 14.8458 \t epsilon: 0.1719820880665765\n",
            "episode: 112 / 1000 \t mean score: -108.94681608216558 \t mean distance: -8.94681608216557 \t mean loss: 15.551193 \t epsilon: 0.1702665667381124\n",
            "episode: 114 / 1000 \t mean score: -122.03125582871647 \t mean distance: -22.031255828716468 \t mean loss: 19.996056 \t epsilon: 0.16856815773489972\n",
            "episode: 116 / 1000 \t mean score: -119.22333167525656 \t mean distance: -19.22333167525656 \t mean loss: 20.531408 \t epsilon: 0.1668866903614941\n",
            "episode: 118 / 1000 \t mean score: -121.30720477557026 \t mean distance: -21.307204775570263 \t mean loss: 30.137695 \t epsilon: 0.16522199562513817\n",
            "episode: 120 / 1000 \t mean score: -122.06572183436057 \t mean distance: -22.065721834360584 \t mean loss: 21.625734 \t epsilon: 0.16357390621877743\n",
            "episode: 122 / 1000 \t mean score: -100.76016645667018 \t mean distance: -0.76016645667018 \t mean loss: 24.104864 \t epsilon: 0.16194225650424512\n",
            "episode: 124 / 1000 \t mean score: -126.6279788808872 \t mean distance: -26.627978880887206 \t mean loss: 22.974522 \t epsilon: 0.16032688249561527\n",
            "episode: 126 / 1000 \t mean score: -110.50178790078218 \t mean distance: -10.501787900782173 \t mean loss: 26.624031 \t epsilon: 0.15872762184272152\n",
            "episode: 128 / 1000 \t mean score: -112.10861768623502 \t mean distance: -12.108617686235018 \t mean loss: 24.186255 \t epsilon: 0.15714431381484037\n",
            "episode: 130 / 1000 \t mean score: -109.89951336064436 \t mean distance: -9.899513360644368 \t mean loss: 13.799455 \t epsilon: 0.15557679928453735\n",
            "episode: 132 / 1000 \t mean score: -112.15030077500859 \t mean distance: -12.150300775008578 \t mean loss: 28.643696 \t epsilon: 0.15402492071167412\n",
            "episode: 134 / 1000 \t mean score: -111.46446684809555 \t mean distance: -11.464466848095551 \t mean loss: 30.728638 \t epsilon: 0.15248852212757516\n",
            "episode: 136 / 1000 \t mean score: -111.3586138112073 \t mean distance: -11.358613811207299 \t mean loss: 20.859943 \t epsilon: 0.1509674491193526\n",
            "episode: 138 / 1000 \t mean score: -100.04564284596374 \t mean distance: -0.045642845963740866 \t mean loss: 22.703718 \t epsilon: 0.14946154881438703\n",
            "episode: 140 / 1000 \t mean score: -121.1589663425656 \t mean distance: -21.158966342565602 \t mean loss: 21.552649 \t epsilon: 0.14797066986496352\n",
            "episode: 142 / 1000 \t mean score: -119.56895370474427 \t mean distance: -19.568953704744263 \t mean loss: 15.675101 \t epsilon: 0.1464946624330605\n",
            "episode: 144 / 1000 \t mean score: -114.41227556498811 \t mean distance: -14.41227556498812 \t mean loss: 23.075394 \t epsilon: 0.14503337817529072\n",
            "episode: 146 / 1000 \t mean score: -111.43848721768862 \t mean distance: -11.438487217688618 \t mean loss: 38.88175 \t epsilon: 0.14358667022799218\n",
            "episode: 148 / 1000 \t mean score: -118.06171755251378 \t mean distance: -18.06171755251379 \t mean loss: 19.46552 \t epsilon: 0.14215439319246798\n",
            "episode: 150 / 1000 \t mean score: -108.51450535459381 \t mean distance: -8.514505354593801 \t mean loss: 25.937016 \t epsilon: 0.1407364031203731\n",
            "episode: 152 / 1000 \t mean score: -105.55542189727194 \t mean distance: -5.555421897271932 \t mean loss: 21.286076 \t epsilon: 0.1393325574992474\n",
            "episode: 154 / 1000 \t mean score: -122.37203152256242 \t mean distance: -22.372031522562434 \t mean loss: 25.281609 \t epsilon: 0.13794271523819238\n",
            "episode: 156 / 1000 \t mean score: -102.87414722856911 \t mean distance: -2.874147228569104 \t mean loss: 21.512539 \t epsilon: 0.1365667366536914\n",
            "episode: 158 / 1000 \t mean score: -101.23359066059345 \t mean distance: -1.2335906605934526 \t mean loss: 15.340592 \t epsilon: 0.13520448345557082\n",
            "episode: 160 / 1000 \t mean score: -101.55967073706289 \t mean distance: -1.5596707370628917 \t mean loss: 24.499512 \t epsilon: 0.1338558187331015\n",
            "episode: 162 / 1000 \t mean score: -125.48092572958703 \t mean distance: -25.480925729587018 \t mean loss: 26.915806 \t epsilon: 0.13252060694123882\n",
            "episode: 164 / 1000 \t mean score: -109.66000487353558 \t mean distance: -9.660004873535588 \t mean loss: 19.76573 \t epsilon: 0.13119871388699997\n",
            "episode: 166 / 1000 \t mean score: -100.31829900365042 \t mean distance: -0.3182990036504343 \t mean loss: 15.911977 \t epsilon: 0.12989000671597714\n",
            "episode: 168 / 1000 \t mean score: -100.65581814231413 \t mean distance: -0.655818142314132 \t mean loss: 27.068956 \t epsilon: 0.12859435389898527\n",
            "episode: 170 / 1000 \t mean score: -122.74895861375796 \t mean distance: -22.748958613757956 \t mean loss: 23.351074 \t epsilon: 0.12731162521884287\n",
            "episode: 172 / 1000 \t mean score: -118.33428961864078 \t mean distance: -18.33428961864078 \t mean loss: 30.88251 \t epsilon: 0.1260416917572849\n",
            "episode: 174 / 1000 \t mean score: -105.44429603002743 \t mean distance: -5.444296030027436 \t mean loss: 17.766197 \t epsilon: 0.124784425882006\n",
            "episode: 176 / 1000 \t mean score: -108.87982165560808 \t mean distance: -8.87982165560809 \t mean loss: 19.105429 \t epsilon: 0.12353970123383298\n",
            "episode: 178 / 1000 \t mean score: -111.2243081881494 \t mean distance: -11.224308188149386 \t mean loss: 28.632027 \t epsilon: 0.1223073927140255\n",
            "episode: 180 / 1000 \t mean score: -113.47310403421146 \t mean distance: -13.473104034211463 \t mean loss: 34.923534 \t epsilon: 0.1210873764717031\n",
            "episode: 182 / 1000 \t mean score: -110.6739474799518 \t mean distance: -10.673947479951808 \t mean loss: 23.249754 \t epsilon: 0.11987952989139786\n",
            "episode: 184 / 1000 \t mean score: -105.60562064871003 \t mean distance: -5.605620648710028 \t mean loss: 30.81592 \t epsilon: 0.11868373158073116\n",
            "episode: 186 / 1000 \t mean score: -123.4813017730036 \t mean distance: -23.481301773003594 \t mean loss: 20.612436 \t epsilon: 0.11749986135821336\n",
            "episode: 188 / 1000 \t mean score: -115.53243243739905 \t mean distance: -15.532432437399052 \t mean loss: 25.84833 \t epsilon: 0.11632780024116518\n",
            "episode: 190 / 1000 \t mean score: -121.27045862630163 \t mean distance: -21.27045862630164 \t mean loss: 22.535824 \t epsilon: 0.11516743043375956\n",
            "episode: 192 / 1000 \t mean score: -114.44582469619247 \t mean distance: -14.445824696192474 \t mean loss: 19.251982 \t epsilon: 0.1140186353151828\n",
            "episode: 194 / 1000 \t mean score: -109.52402320089502 \t mean distance: -9.524023200895007 \t mean loss: 25.680813 \t epsilon: 0.11288129942791385\n",
            "episode: 196 / 1000 \t mean score: -121.09542704001908 \t mean distance: -21.095427040019068 \t mean loss: 22.463554 \t epsilon: 0.1117553084661204\n",
            "episode: 198 / 1000 \t mean score: -119.92778836726521 \t mean distance: -19.9277883672652 \t mean loss: 25.138391 \t epsilon: 0.11064054926417084\n",
            "episode: 200 / 1000 \t mean score: -123.59361709511683 \t mean distance: -23.593617095116826 \t mean loss: 31.496683 \t epsilon: 0.10953690978526073\n",
            "episode: 202 / 1000 \t mean score: -112.98788983695613 \t mean distance: -12.987889836956114 \t mean loss: 31.752827 \t epsilon: 0.10844427911015277\n",
            "episode: 204 / 1000 \t mean score: -98.83241397240911 \t mean distance: 1.1675860275908774 \t mean loss: 25.882187 \t epsilon: 0.10736254742602899\n",
            "episode: 206 / 1000 \t mean score: -156.54928593866575 \t mean distance: -56.54928593866574 \t mean loss: 23.582895 \t epsilon: 0.10629160601545436\n",
            "episode: 208 / 1000 \t mean score: -122.94983907484573 \t mean distance: -22.94983907484572 \t mean loss: 23.484219 \t epsilon: 0.1052313472454502\n",
            "episode: 210 / 1000 \t mean score: -122.32984663143412 \t mean distance: -22.329846631434116 \t mean loss: 15.638739 \t epsilon: 0.10418166455667685\n",
            "episode: 212 / 1000 \t mean score: -103.16870360022844 \t mean distance: -3.1687036002284317 \t mean loss: 27.709122 \t epsilon: 0.10314245245272399\n",
            "episode: 214 / 1000 \t mean score: -113.11490985408392 \t mean distance: -13.114909854083926 \t mean loss: 19.192966 \t epsilon: 0.10211360648950807\n",
            "episode: 216 / 1000 \t mean score: -113.32774794755315 \t mean distance: -13.327747947553158 \t mean loss: 24.402271 \t epsilon: 0.10109502326477522\n",
            "episode: 218 / 1000 \t mean score: -103.73435540847697 \t mean distance: -3.7343554084769672 \t mean loss: 27.198727 \t epsilon: 0.10008660040770909\n",
            "episode: 220 / 1000 \t mean score: -127.1366102539723 \t mean distance: -27.136610253972297 \t mean loss: 21.487877 \t epsilon: 0.09908823656864219\n",
            "episode: 222 / 1000 \t mean score: -108.20255597156998 \t mean distance: -8.202555971569986 \t mean loss: 25.25805 \t epsilon: 0.09809983140886998\n",
            "episode: 224 / 1000 \t mean score: -102.43194025356165 \t mean distance: -2.4319402535616517 \t mean loss: 26.24751 \t epsilon: 0.0971212855905665\n",
            "episode: 226 / 1000 \t mean score: -111.54112928386836 \t mean distance: -11.541129283868363 \t mean loss: 22.582804 \t epsilon: 0.0961525007668006\n",
            "episode: 228 / 1000 \t mean score: -123.74139273563678 \t mean distance: -23.741392735636776 \t mean loss: 30.894073 \t epsilon: 0.09519337957165176\n",
            "episode: 230 / 1000 \t mean score: -122.90077311681519 \t mean distance: -22.90077311681519 \t mean loss: 18.756834 \t epsilon: 0.09424382561042453\n",
            "episode: 232 / 1000 \t mean score: -110.33478054783649 \t mean distance: -10.334780547836479 \t mean loss: 14.457638 \t epsilon: 0.09330374344996055\n",
            "episode: 234 / 1000 \t mean score: -123.51525387648833 \t mean distance: -23.51525387648833 \t mean loss: 27.615307 \t epsilon: 0.09237303860904719\n",
            "episode: 236 / 1000 \t mean score: -123.51856751478556 \t mean distance: -23.518567514785552 \t mean loss: 32.63568 \t epsilon: 0.09145161754892195\n",
            "episode: 238 / 1000 \t mean score: -126.20229018004486 \t mean distance: -26.202290180044855 \t mean loss: 12.908829 \t epsilon: 0.09053938766387146\n",
            "episode: 240 / 1000 \t mean score: -107.66736190666259 \t mean distance: -7.66736190666258 \t mean loss: 24.862028 \t epsilon: 0.08963625727192434\n",
            "episode: 242 / 1000 \t mean score: -110.42448188451087 \t mean distance: -10.424481884510863 \t mean loss: 25.720915 \t epsilon: 0.0887421356056369\n",
            "episode: 244 / 1000 \t mean score: -109.18333297544126 \t mean distance: -9.183332975441264 \t mean loss: 31.761297 \t epsilon: 0.08785693280297067\n",
            "episode: 246 / 1000 \t mean score: -121.99287274798439 \t mean distance: -21.992872747984396 \t mean loss: 30.806498 \t epsilon: 0.08698055989826103\n",
            "episode: 248 / 1000 \t mean score: -119.96785131128689 \t mean distance: -19.9678513112869 \t mean loss: 38.726433 \t epsilon: 0.08611292881327588\n",
            "episode: 250 / 1000 \t mean score: -99.34619702061235 \t mean distance: 0.6538029793876547 \t mean loss: 21.539785 \t epsilon: 0.08525395234836346\n",
            "episode: 252 / 1000 \t mean score: -110.47414909170723 \t mean distance: -10.474149091707229 \t mean loss: 27.961319 \t epsilon: 0.08440354417368853\n",
            "episode: 254 / 1000 \t mean score: -109.38170498615781 \t mean distance: -9.381704986157802 \t mean loss: 31.946554 \t epsilon: 0.08356161882055599\n",
            "episode: 256 / 1000 \t mean score: -110.94023130600463 \t mean distance: -10.940231306004629 \t mean loss: 21.51892 \t epsilon: 0.08272809167282094\n",
            "episode: 258 / 1000 \t mean score: -123.77388667588306 \t mean distance: -23.773886675883062 \t mean loss: 17.956112 \t epsilon: 0.08190287895838455\n",
            "episode: 260 / 1000 \t mean score: -103.1284459350947 \t mean distance: -3.1284459350946974 \t mean loss: 16.662706 \t epsilon: 0.08108589774077467\n",
            "episode: 262 / 1000 \t mean score: -122.74585290615786 \t mean distance: -22.74585290615787 \t mean loss: 24.425812 \t epsilon: 0.08027706591081045\n",
            "episode: 264 / 1000 \t mean score: -120.34562820664887 \t mean distance: -20.345628206648865 \t mean loss: 21.028988 \t epsilon: 0.07947630217835011\n",
            "episode: 266 / 1000 \t mean score: -121.08226493269888 \t mean distance: -21.08226493269887 \t mean loss: 20.4903 \t epsilon: 0.07868352606412107\n",
            "episode: 268 / 1000 \t mean score: -120.07059684294491 \t mean distance: -20.070596842944912 \t mean loss: 20.095886 \t epsilon: 0.07789865789163146\n",
            "episode: 270 / 1000 \t mean score: -121.56017509806343 \t mean distance: -21.560175098063418 \t mean loss: 30.201965 \t epsilon: 0.07712161877916243\n",
            "episode: 272 / 1000 \t mean score: -123.29164882667018 \t mean distance: -23.291648826670176 \t mean loss: 27.673656 \t epsilon: 0.07635233063184028\n",
            "episode: 274 / 1000 \t mean score: -132.04804276188582 \t mean distance: -32.04804276188581 \t mean loss: 19.751417 \t epsilon: 0.07559071613378768\n",
            "episode: 276 / 1000 \t mean score: -109.3473837553502 \t mean distance: -9.347383755350187 \t mean loss: 42.878082 \t epsilon: 0.07483669874035313\n",
            "episode: 278 / 1000 \t mean score: -100.11542882391004 \t mean distance: -0.11542882391003761 \t mean loss: 22.095184 \t epsilon: 0.07409020267041812\n",
            "episode: 280 / 1000 \t mean score: -99.27688186811966 \t mean distance: 0.7231181318803332 \t mean loss: 22.730745 \t epsilon: 0.07335115289878069\n",
            "episode: 282 / 1000 \t mean score: -111.26824318546801 \t mean distance: -11.268243185468021 \t mean loss: 28.619179 \t epsilon: 0.07261947514861535\n",
            "episode: 284 / 1000 \t mean score: -101.75085606245139 \t mean distance: -1.750856062451384 \t mean loss: 23.521713 \t epsilon: 0.0718950958840079\n",
            "episode: 286 / 1000 \t mean score: -120.52035629161162 \t mean distance: -20.520356291611616 \t mean loss: 20.321892 \t epsilon: 0.07117794230256491\n",
            "episode: 288 / 1000 \t mean score: -113.29118564083271 \t mean distance: -13.291185640832712 \t mean loss: 24.135359 \t epsilon: 0.07046794232809683\n",
            "episode: 290 / 1000 \t mean score: -113.1350140741275 \t mean distance: -13.1350140741275 \t mean loss: 26.78222 \t epsilon: 0.06976502460337407\n",
            "episode: 292 / 1000 \t mean score: -123.94310150531709 \t mean distance: -23.943101505317088 \t mean loss: 23.090628 \t epsilon: 0.06906911848295541\n",
            "episode: 294 / 1000 \t mean score: -121.42756883167723 \t mean distance: -21.42756883167724 \t mean loss: 25.186153 \t epsilon: 0.06838015402608792\n",
            "episode: 296 / 1000 \t mean score: -122.60156183707993 \t mean distance: -22.601561837079927 \t mean loss: 13.081142 \t epsilon: 0.06769806198967769\n",
            "episode: 298 / 1000 \t mean score: -120.41159170895381 \t mean distance: -20.411591708953807 \t mean loss: 31.567999 \t epsilon: 0.06702277382133066\n",
            "episode: 300 / 1000 \t mean score: -139.25306002204462 \t mean distance: -39.25306002204461 \t mean loss: 24.935293 \t epsilon: 0.06635422165246288\n",
            "episode: 302 / 1000 \t mean score: -123.29037964468354 \t mean distance: -23.29037964468354 \t mean loss: 23.335217 \t epsilon: 0.06569233829147957\n",
            "episode: 304 / 1000 \t mean score: -119.56828232317758 \t mean distance: -19.568282323177584 \t mean loss: 18.313906 \t epsilon: 0.06503705721702206\n",
            "episode: 306 / 1000 \t mean score: -112.91754904697028 \t mean distance: -12.917549046970278 \t mean loss: 27.102291 \t epsilon: 0.06438831257128227\n",
            "episode: 308 / 1000 \t mean score: -124.42373861272631 \t mean distance: -24.423738612726314 \t mean loss: 24.208364 \t epsilon: 0.06374603915338374\n",
            "episode: 310 / 1000 \t mean score: -123.5852932173917 \t mean distance: -23.585293217391705 \t mean loss: 20.344355 \t epsilon: 0.06311017241282874\n",
            "episode: 312 / 1000 \t mean score: -122.28073764392548 \t mean distance: -22.28073764392547 \t mean loss: 35.57211 \t epsilon: 0.062480648443010774\n",
            "episode: 314 / 1000 \t mean score: -112.65437542741776 \t mean distance: -12.65437542741776 \t mean loss: 31.95605 \t epsilon: 0.06185740397479174\n",
            "episode: 316 / 1000 \t mean score: -125.3850792124175 \t mean distance: -25.385079212417494 \t mean loss: 25.147045 \t epsilon: 0.061240376370143186\n",
            "episode: 318 / 1000 \t mean score: -127.04317239325584 \t mean distance: -27.04317239325585 \t mean loss: 28.289013 \t epsilon: 0.06062950361585101\n",
            "episode: 320 / 1000 \t mean score: -122.95815534682879 \t mean distance: -22.958155346828796 \t mean loss: 19.610346 \t epsilon: 0.060024724317282896\n",
            "episode: 322 / 1000 \t mean score: -110.21939370671318 \t mean distance: -10.219393706713175 \t mean loss: 27.599348 \t epsilon: 0.059425977692218\n",
            "episode: 324 / 1000 \t mean score: -105.63161292192058 \t mean distance: -5.631612921920574 \t mean loss: 30.660812 \t epsilon: 0.05883320356473813\n",
            "episode: 326 / 1000 \t mean score: -133.14945195173752 \t mean distance: -33.14945195173753 \t mean loss: 17.12574 \t epsilon: 0.058246342359179866\n",
            "episode: 328 / 1000 \t mean score: -111.20460541877492 \t mean distance: -11.204605418774912 \t mean loss: 22.69855 \t epsilon: 0.05766533509414704\n",
            "episode: 330 / 1000 \t mean score: -140.71509406526505 \t mean distance: -90.71509406526505 \t mean loss: 23.739399 \t epsilon: 0.057090123376582924\n",
            "episode: 332 / 1000 \t mean score: -122.78119094191676 \t mean distance: -22.781190941916766 \t mean loss: 28.705442 \t epsilon: 0.05652064939590151\n",
            "episode: 334 / 1000 \t mean score: -122.10953225545367 \t mean distance: -22.109532255453672 \t mean loss: 32.437946 \t epsilon: 0.055956855918177395\n",
            "episode: 336 / 1000 \t mean score: -123.37925610438423 \t mean distance: -23.379256104384226 \t mean loss: 24.798582 \t epsilon: 0.05539868628039357\n",
            "episode: 338 / 1000 \t mean score: -176.40612798929197 \t mean distance: -76.40612798929199 \t mean loss: 17.349762 \t epsilon: 0.05484608438474665\n",
            "episode: 340 / 1000 \t mean score: -121.85902181957569 \t mean distance: -21.859021819575688 \t mean loss: 27.308725 \t epsilon: 0.0542989946930088\n",
            "episode: 342 / 1000 \t mean score: -105.07738819330989 \t mean distance: -5.077388193309889 \t mean loss: 23.803722 \t epsilon: 0.053757362220946035\n",
            "episode: 344 / 1000 \t mean score: -111.735589906862 \t mean distance: -11.735589906861994 \t mean loss: 23.752462 \t epsilon: 0.0532211325327921\n",
            "episode: 346 / 1000 \t mean score: -104.05288119949678 \t mean distance: -4.05288119949678 \t mean loss: 11.859191 \t epsilon: 0.052690251735777495\n",
            "episode: 348 / 1000 \t mean score: -124.90194060775576 \t mean distance: -24.901940607755762 \t mean loss: 19.7555 \t epsilon: 0.052164666474713114\n",
            "episode: 350 / 1000 \t mean score: -126.5067873165859 \t mean distance: -26.506787316585886 \t mean loss: 30.142769 \t epsilon: 0.05164432392662785\n",
            "episode: 352 / 1000 \t mean score: -111.55599470772344 \t mean distance: -11.555994707723444 \t mean loss: 26.555016 \t epsilon: 0.051129171795459734\n",
            "episode: 354 / 1000 \t mean score: -123.09709312095646 \t mean distance: -23.097093120956462 \t mean loss: 17.511862 \t epsilon: 0.05061915830680002\n",
            "episode: 356 / 1000 \t mean score: -123.92490265132146 \t mean distance: -23.924902651321453 \t mean loss: 27.27761 \t epsilon: 0.05011423220268969\n",
            "episode: 358 / 1000 \t mean score: -99.16551539884725 \t mean distance: 0.8344846011527458 \t mean loss: 20.391592 \t epsilon: 0.04961434273646786\n",
            "episode: 360 / 1000 \t mean score: -100.8771229698748 \t mean distance: -0.8771229698748063 \t mean loss: 31.59 \t epsilon: 0.04911943966767159\n",
            "episode: 362 / 1000 \t mean score: -112.55249814442483 \t mean distance: -12.552498144424833 \t mean loss: 22.942215 \t epsilon: 0.04862947325698656\n",
            "episode: 364 / 1000 \t mean score: -110.39064895872896 \t mean distance: -10.390648958728956 \t mean loss: 24.145529 \t epsilon: 0.04814439426124812\n",
            "episode: 366 / 1000 \t mean score: -111.05491547135293 \t mean distance: -11.05491547135293 \t mean loss: 28.364906 \t epsilon: 0.047664153928492166\n",
            "episode: 368 / 1000 \t mean score: -100.9394806385711 \t mean distance: -0.9394806385710852 \t mean loss: 22.119915 \t epsilon: 0.047188703993055456\n",
            "episode: 370 / 1000 \t mean score: -99.77363980193871 \t mean distance: 0.22636019806128993 \t mean loss: 37.700443 \t epsilon: 0.04671799667072473\n",
            "episode: 372 / 1000 \t mean score: -100.19784079846833 \t mean distance: -0.19784079846833094 \t mean loss: 28.79114 \t epsilon: 0.04625198465393425\n",
            "episode: 374 / 1000 \t mean score: -117.76189551099358 \t mean distance: -17.761895510993597 \t mean loss: 28.7451 \t epsilon: 0.04579062110701125\n",
            "episode: 376 / 1000 \t mean score: -126.3061739794522 \t mean distance: -26.3061739794522 \t mean loss: 24.3758 \t epsilon: 0.045333859661468814\n",
            "episode: 378 / 1000 \t mean score: -125.61473416593796 \t mean distance: -25.614734165937964 \t mean loss: 26.818422 \t epsilon: 0.044881654411345666\n",
            "episode: 380 / 1000 \t mean score: -125.30723182182666 \t mean distance: -25.30723182182667 \t mean loss: 26.304617 \t epsilon: 0.044433959908592495\n",
            "episode: 382 / 1000 \t mean score: -127.74456016028455 \t mean distance: -27.744560160284557 \t mean loss: 22.525604 \t epsilon: 0.043990731158504284\n",
            "episode: 384 / 1000 \t mean score: -127.01982832719014 \t mean distance: -27.01982832719014 \t mean loss: 22.98799 \t epsilon: 0.043551923615198204\n",
            "episode: 386 / 1000 \t mean score: -128.27068640055694 \t mean distance: -28.27068640055694 \t mean loss: 20.997107 \t epsilon: 0.043117493177136604\n",
            "episode: 388 / 1000 \t mean score: -113.8338096906757 \t mean distance: -13.833809690675695 \t mean loss: 24.198164 \t epsilon: 0.04268739618269467\n",
            "episode: 390 / 1000 \t mean score: -112.77033489260359 \t mean distance: -12.770334892603579 \t mean loss: 25.858637 \t epsilon: 0.042261589405772286\n",
            "episode: 392 / 1000 \t mean score: -113.7385766552544 \t mean distance: -13.738576655254398 \t mean loss: 26.195614 \t epsilon: 0.04184003005144971\n",
            "episode: 394 / 1000 \t mean score: -116.08124562499385 \t mean distance: -16.081245624993855 \t mean loss: 19.811934 \t epsilon: 0.0414226757516865\n",
            "episode: 396 / 1000 \t mean score: -126.89605941398315 \t mean distance: -26.896059413983153 \t mean loss: 25.32588 \t epsilon: 0.041009484561063424\n",
            "episode: 398 / 1000 \t mean score: -116.68101365888646 \t mean distance: -16.681013658886446 \t mean loss: 19.999907 \t epsilon: 0.04060041495256681\n",
            "episode: 400 / 1000 \t mean score: -110.07055262547607 \t mean distance: -10.070552625476063 \t mean loss: 18.82382 \t epsilon: 0.04019542581341495\n",
            "episode: 402 / 1000 \t mean score: -102.51625067014774 \t mean distance: -2.516250670147732 \t mean loss: 21.400906 \t epsilon: 0.03979447644092614\n",
            "episode: 404 / 1000 \t mean score: -125.29395219887107 \t mean distance: -25.293952198871082 \t mean loss: 30.324701 \t epsilon: 0.0393975265384279\n",
            "episode: 406 / 1000 \t mean score: -100.61934651816543 \t mean distance: -0.6193465181654205 \t mean loss: 19.699234 \t epsilon: 0.039004536211207086\n",
            "episode: 408 / 1000 \t mean score: -101.6052534994014 \t mean distance: -1.6052534994013972 \t mean loss: 25.695452 \t epsilon: 0.038615465962500296\n",
            "episode: 410 / 1000 \t mean score: -123.383924703449 \t mean distance: -23.383924703448997 \t mean loss: 28.8755 \t epsilon: 0.038230276689524355\n",
            "episode: 412 / 1000 \t mean score: -102.41585137316864 \t mean distance: -2.4158513731686413 \t mean loss: 29.096933 \t epsilon: 0.037848929679546346\n",
            "episode: 414 / 1000 \t mean score: -114.49672698192302 \t mean distance: -64.49672698192302 \t mean loss: 23.17629 \t epsilon: 0.037471386605992876\n",
            "episode: 416 / 1000 \t mean score: -113.55060208203236 \t mean distance: -13.550602082032347 \t mean loss: 22.062366 \t epsilon: 0.0370976095245981\n",
            "episode: 418 / 1000 \t mean score: -112.3817911524828 \t mean distance: -12.381791152482798 \t mean loss: 15.103473 \t epsilon: 0.036727560869590226\n",
            "episode: 420 / 1000 \t mean score: -123.58143580039123 \t mean distance: -23.581435800391226 \t mean loss: 21.782326 \t epsilon: 0.036361203449916064\n",
            "episode: 422 / 1000 \t mean score: -127.84371219948397 \t mean distance: -27.84371219948399 \t mean loss: 19.36425 \t epsilon: 0.03599850044550315\n",
            "episode: 424 / 1000 \t mean score: -112.21252572820643 \t mean distance: -12.212525728206431 \t mean loss: 24.084946 \t epsilon: 0.035639415403559256\n",
            "episode: 426 / 1000 \t mean score: -135.05021169844795 \t mean distance: -35.05021169844797 \t mean loss: 25.181768 \t epsilon: 0.035283912234908746\n",
            "episode: 428 / 1000 \t mean score: -112.647164144642 \t mean distance: -12.647164144642 \t mean loss: 24.102448 \t epsilon: 0.03493195521036553\n",
            "episode: 430 / 1000 \t mean score: -122.05507458875358 \t mean distance: -22.055074588753584 \t mean loss: 16.409777 \t epsilon: 0.034583508957142135\n",
            "episode: 432 / 1000 \t mean score: -111.94320388607461 \t mean distance: -11.943203886074613 \t mean loss: 21.878828 \t epsilon: 0.03423853845529464\n",
            "episode: 434 / 1000 \t mean score: -127.84753165678305 \t mean distance: -27.84753165678304 \t mean loss: 22.32776 \t epsilon: 0.03389700903420308\n",
            "episode: 436 / 1000 \t mean score: -110.74598786168048 \t mean distance: -10.74598786168048 \t mean loss: 30.114183 \t epsilon: 0.033558886369086896\n",
            "episode: 438 / 1000 \t mean score: -122.58943596575563 \t mean distance: -22.589435965755623 \t mean loss: 34.043587 \t epsilon: 0.033224136477555254\n",
            "episode: 440 / 1000 \t mean score: -123.07624163456012 \t mean distance: -23.076241634560112 \t mean loss: 15.625312 \t epsilon: 0.03289272571619164\n",
            "episode: 442 / 1000 \t mean score: -114.13811930667143 \t mean distance: -14.138119306671424 \t mean loss: 24.878544 \t epsilon: 0.03256462077717262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoRFdJH1u4CC"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXe3NShru4CD"
      },
      "outputs": [],
      "source": [
        "print(\"Total time: \", time.time() - stime)\n",
        "\n",
        "x = [idx + 1 for idx in range(len(scores))]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, scores)\n",
        "plt.title('Scores [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(scorefigname)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4tPHG0saO2D"
      },
      "outputs": [],
      "source": [
        "c_reward = np.cumsum(scores)\n",
        "c_indices = np.arange(len(c_reward))\n",
        "rolling_avg = c_reward / c_indices\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, rolling_avg)\n",
        "plt.title('Average reward [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(cumfigname)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByFXvH2Mu4CE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, eps_history)\n",
        "plt.title('Epsilon history [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(epsfigname)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(np.arange(len(distances)), distances)\n",
        "plt.title('Distances [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(distfigname)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hs-oS5UrE8sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_distances = np.cumsum(distances)\n",
        "c_indices_distances = np.arange(len(c_distances))\n",
        "rolling_avg_distances = c_distances / c_indices_distances\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, rolling_avg_distances)\n",
        "plt.title('Mean distance [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(meandistfigname)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a3NPuueUFRPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHnAgjMGu4CE"
      },
      "source": [
        "# Log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU1BATVPu4CF"
      },
      "outputs": [],
      "source": [
        "dfname = datename + '_runlogs'\n",
        "df_path = 'logs/' + env_name_clean + '_' + dfname + '.csv'\n",
        "df_avg_path = 'logs/' + env_name_clean + '_' + dfname + '_avg.csv'\n",
        "df_dist_path = 'logs/' + env_name_clean + '_' + dfname + '_distances.csv'\n",
        "\n",
        "df = pd.DataFrame({'score': scores, 'Epsilon': eps_history})\n",
        "df_avg = pd.DataFrame({'avg_score': avg_scores})\n",
        "df_dist = pd.DataFrame({'distances': distances})\n",
        "\n",
        "df.to_csv(df_path, sep = ';', header = True, index = False)\n",
        "df_avg.to_csv(df_avg_path, sep = ';', header = True, index = False)\n",
        "df_dist.to_csv(df_dist_path, sep = ';', header = True, index = False)\n",
        "\n",
        "print(\"Done logging results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76pGlcaiu4CH"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTEKGkC6u4CI"
      },
      "outputs": [],
      "source": [
        "print('Starting competitive trials...')\n",
        "compscores = 0\n",
        "agent.load_models()\n",
        "print('epsilon', epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjCoTRAtu4CI"
      },
      "outputs": [],
      "source": [
        "for i in range(n_rounds):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        env.render()\n",
        "        \n",
        "        action = agent.act(state, epsilon)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        agent.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        state = next_state\n",
        "        score += reward\n",
        "    \n",
        "    compscores += score\n",
        "    print(\"Competitive round \", i + 1, \" Overall score \", compscores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzg3fZnYu4CI"
      },
      "source": [
        "# Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdr_4ZppOJs3"
      },
      "outputs": [],
      "source": [
        "with open(outfilename, \"w\") as f:\n",
        "    f.writelines(\"%s: %i\\n\" % (env_name_clean, compscores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqaiDFmKOJWg"
      },
      "outputs": [],
      "source": [
        "if(mode == 'colab'):\n",
        "    from google.colab import files\n",
        "    files.download(model_file)\n",
        "    files.download(model_file_target)\n",
        "    files.download(outfilename)\n",
        "    files.download(df_path)\n",
        "    files.download(df_avg_path)\n",
        "    files.download(df_dist_path)\n",
        "    files.download('images/' + scorefigname)\n",
        "    files.download('images/' + epsfigname)\n",
        "    files.download('images/' + cumfigname)\n",
        "    files.download('images/' + distfigname)\n",
        "    files.download('images/' + meandistfigname)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}