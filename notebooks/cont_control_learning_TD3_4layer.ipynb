{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Directories"
      ],
      "metadata": {
        "id": "l8Pk2GDiu4Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "mode = -1\n",
        "mode = 'local'\n",
        "# Install necessary files and create necessary folders (Only when ran in Google Drive)\n",
        "!pip install snntorch==0.5.3\n",
        "!pip install gym==0.25.2\n",
        "!pip install box2d-py==2.3.5\n",
        "!pip install torch==1.12.1+cu113\n",
        "!pip install \n",
        "!pip install swig\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "#!pip install gym[box2d]\n",
        "!mkdir tmp\n",
        "!mkdir logs\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#!cp \"/content/drive/MyDrive/Colab Notebooks/dict9 LIF\" \"/content/tmp/\"\n",
        "mode = 'colab'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: snntorch==0.5.3 in /usr/local/lib/python3.8/dist-packages (0.5.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from snntorch==0.5.3) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from snntorch==0.5.3) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from snntorch==0.5.3) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from snntorch==0.5.3) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.1.0->snntorch==0.5.3) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->snntorch==0.5.3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->snntorch==0.5.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->snntorch==0.5.3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->snntorch==0.5.3) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->snntorch==0.5.3) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->snntorch==0.5.3) (2022.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.25.2) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.25.2) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.25.2) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.25.2) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym==0.25.2) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.8/dist-packages (2.3.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.12.1+cu113 in /usr/local/lib/python3.8/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.1+cu113) (4.1.1)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.8/dist-packages (4.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.1.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.10.0)\n",
            "mkdir: cannot create directory ‘tmp’: File exists\n",
            "mkdir: cannot create directory ‘logs’: File exists\n"
          ]
        }
      ],
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EyOhCiEu4Bk",
        "outputId": "a79af4f7-7ac2-48b3-89e1-d54288242797",
        "gather": {
          "logged": 1669830070564
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "hv0oAX5su4Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "from datetime import datetime\n",
        "from collections import deque, namedtuple\n",
        "from gym import wrappers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.cuda.memory import mem_get_info\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import surrogate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "id": "YaOc25LDu4Br",
        "gather": {
          "logged": 1669830072014
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A small method to save plots\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=resolution)"
      ],
      "outputs": [],
      "execution_count": 57,
      "metadata": {
        "id": "1W8CBfWbvZM8",
        "gather": {
          "logged": 1669830072481
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters ###"
      ],
      "metadata": {
        "id": "DtnGruUvu4Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 5e-4                   # Learning rate # lr = 0.001\n",
        "batch_size = 128\n",
        "gauss_per_dim = 64\n",
        "max_episodes = 1000         # max num of episodes\n",
        "max_timesteps = 2000        # max timesteps in one episode\n",
        "num_steps = 16              # How many steps to loop in prediction\n",
        "n_rounds = 5                # Number of competitive rounds to play\n",
        "\n",
        "log_interval = 100          # print avg reward after interval\n",
        "random_seed = 0\n",
        "gamma = 0.99                # discount for future rewards\n",
        "batch_size = 16            # num of transitions sampled from replay buffer\n",
        "exploration_noise = 0.1\n",
        "polyak = 0.995              # target policy update parameter (1-tau)\n",
        "policy_noise = 0.2          # target policy smoothing noise\n",
        "noise_clip = 0.5\n",
        "policy_delay = 2            # delayed policy updates parameter\n",
        "start_episode = 0"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "id": "uSJzX05Vu4B1",
        "gather": {
          "logged": 1669830073103
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'BipedalWalker-v3'\n",
        "env_name_clean = env_name.lower().replace('-','_')\n",
        "filename = env_name_clean + \".png\"\n",
        "outfilename = env_name_clean + \"scoreboard.txt\"\n",
        "datename = str(datetime.now()).split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_') \n",
        "scorefigname = env_name_clean + '_' + datename + '_scores.png'\n",
        "epsfigname = env_name_clean + '_' + datename + '_eps_history.png'\n",
        "cumfigname = env_name_clean + '_' + datename + '_cumulated_reward.png'\n",
        "distfigname = env_name_clean + '_' + datename + '_distances.png'\n",
        "meandistfigname = env_name_clean + '_' + datename + '_mean_distances.png'\n",
        "model_file = './tmp/'  + env_name_clean + '_dict9'\n",
        "model_file_target = model_file + '_target'\n",
        "directory = \"./tmp/\"\n",
        "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Env name (clean):',env_name_clean)\n",
        "print('Datetime (clean):', datename)\n",
        "print('Output file:', outfilename)\n",
        "print('Model file:', model_file)\n",
        "print('Running training on: ', device)\n",
        "print('Environment:', mode)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Env name (clean): bipedalwalker_v3\n",
            "Datetime (clean): 2022_12_04_14_08_51\n",
            "Output file: bipedalwalker_v3scoreboard.txt\n",
            "Model file: ./tmp/bipedalwalker_v3_dict9\n",
            "Running training on:  cuda\n",
            "Environment: colab\n"
          ]
        }
      ],
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAqS6F-Yu4By",
        "outputId": "da0856fb-46a2-4e66-f76f-e9c876551708",
        "gather": {
          "logged": 1669830073526
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "wU6d2CLFu4B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor"
      ],
      "metadata": {
        "id": "dlbfSeXDSIiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action, beta=0.95):\n",
        "        super(Actor, self).__init__()\n",
        "        self.max_action = max_action\n",
        "        \n",
        "        # Initialize grad objects        \n",
        "        spike_grad1 = surrogate.fast_sigmoid()\n",
        "        spike_grad2 = surrogate.fast_sigmoid()\n",
        "        spike_grad3 = surrogate.fast_sigmoid()\n",
        "        spike_grad4 = surrogate.fast_sigmoid()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.lif1 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad1)\n",
        "        \n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.lif2 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad2)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.lif3 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad3)\n",
        "\n",
        "        self.fc4 = nn.Linear(64, action_dim)\n",
        "        self.lif4 = snn.Leaky(beta=beta, learn_beta=True, threshold=1e5, reset_mechanism=\"none\", spike_grad=spike_grad4)\n",
        "\n",
        "    def forward(self, x, num_steps=16):\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        # Record the final layer#\n",
        "        spk1_rec = []\n",
        "        mem1_rec = []\n",
        "        \n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "        \n",
        "        spk3_rec = []\n",
        "        mem3_rec = []        \n",
        "\n",
        "        spk4_rec = []\n",
        "        mem4_rec = []        \n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            \n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            \n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            cur4 = self.fc4(spk3)\n",
        "            spk4, mem4 = self.lif4(cur4, mem4)\n",
        "\n",
        "            spk1_rec.append(spk1)\n",
        "            mem1_rec.append(mem1)\n",
        "            \n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "            \n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "            spk4_rec.append(spk4)\n",
        "            mem4_rec.append(mem4)\n",
        "\n",
        "        return torch.stack(mem4_rec, dim=0) * self.max_action"
      ],
      "outputs": [],
      "execution_count": 60,
      "metadata": {
        "id": "P_Kt1Ienp0dx",
        "gather": {
          "logged": 1669830074002
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Critic"
      ],
      "metadata": {
        "id": "J0nnEGXw8YHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, beta=0.95):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Initialize grad objects        \n",
        "        spike_grad1 = surrogate.fast_sigmoid()\n",
        "        spike_grad2 = surrogate.fast_sigmoid()\n",
        "        spike_grad3 = surrogate.fast_sigmoid()\n",
        "        spike_grad4 = surrogate.fast_sigmoid()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(1408, 1024)\n",
        "        self.lif1 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad1)\n",
        "        \n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.lif2 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad2)\n",
        "\n",
        "        self.fc3 = nn.Linear(512, 128)\n",
        "        self.lif3 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad3)\n",
        "\n",
        "        self.fc4 = nn.Linear(128, 1)\n",
        "        self.lif4 = snn.Leaky(beta=beta, learn_beta=True, threshold=1e5, reset_mechanism=\"none\", spike_grad=spike_grad4)\n",
        "\n",
        "    def forward(self, state, action, num_steps=16):\n",
        "        state = state.flatten()\n",
        "        action = action.flatten()\n",
        "        x = torch.cat([state, action], 0)\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        # Record the final layer#\n",
        "        spk1_rec = []\n",
        "        mem1_rec = []\n",
        "        \n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "        \n",
        "        spk3_rec = []\n",
        "        mem3_rec = []        \n",
        "\n",
        "        spk4_rec = []\n",
        "        mem4_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            \n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            \n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            cur4 = self.fc4(spk3)\n",
        "            spk4, mem4 = self.lif4(cur4, mem4)\n",
        "\n",
        "            spk1_rec.append(spk1)\n",
        "            mem1_rec.append(mem1)\n",
        "            \n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "            \n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "            spk4_rec.append(spk4)\n",
        "            mem4_rec.append(mem4)\n",
        "\n",
        "        return torch.stack(mem4_rec, dim=0)"
      ],
      "outputs": [],
      "execution_count": 61,
      "metadata": {
        "id": "9j2tGXDTqkXA",
        "gather": {
          "logged": 1669830074456
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReplayBuffer"
      ],
      "metadata": {
        "id": "eywnQwfISDqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=5e5):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)"
      ],
      "outputs": [],
      "execution_count": 62,
      "metadata": {
        "id": "izTUGxP5QsoG",
        "gather": {
          "logged": 1669830074939
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "W4KED1jNSLIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3:\n",
        "    def __init__(self, lr, state_size, action_size, max_action, low, high, action_low, action_high, gauss_per_dim):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "\n",
        "        critic_low = np.concatenate([low, action_low])         \n",
        "        critic_high = np.concatenate([high, action_high])\n",
        "        critic_input = (state_size + action_size)\n",
        "\n",
        "        self.critic_1 = Critic(critic_input).to(device)\n",
        "        self.critic_1_target = Critic(critic_input).to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_2 = Critic(critic_input).to(device)\n",
        "        self.critic_2_target = Critic(critic_input).to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "\n",
        "        self.max_action = max_action\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "    \n",
        "    def update(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size, 1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "            \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            actor_target_out = self.actor_target(next_state)\n",
        "            noise = torch.reshape(noise, tuple(actor_target_out.shape))\n",
        "            next_action = (actor_target_out + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "\n",
        "        return actor_loss.cpu().data.numpy(), loss_Q1.cpu().data.numpy(), loss_Q2.cpu().data.numpy()\n",
        "                \n",
        "    def save(self, directory, name, ep):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor_ep%s.pth' % (directory, name, ep))\n",
        "        torch.save(self.actor_target.state_dict(), '%s/%s_actor_target_ep%s.pth' % (directory, name, ep))\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), '%s/%s_crtic_1_ep%s.pth' % (directory, name, ep))\n",
        "        torch.save(self.critic_1_target.state_dict(), '%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep))\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), '%s/%s_crtic_2_ep%s.pth' % (directory, name, ep))\n",
        "        torch.save(self.critic_2_target.state_dict(), '%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep))\n",
        "        \n",
        "    def load(self, directory, name, ep):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load('%s/%s_crtic_1_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load('%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load('%s/%s_crtic_2_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load('%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "    def load_actor(self, directory, name, ep):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))"
      ],
      "outputs": [],
      "execution_count": 63,
      "metadata": {
        "id": "-QRt0zUKgmQB",
        "gather": {
          "logged": 1669830076319
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "g6VdDDb3NXPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gym.logger.set_level(40)\n",
        "env_name = \"BipedalWalker-v3\"\n",
        "# env_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "policy = TD3(lr, \n",
        "             state_dim, \n",
        "             action_dim, \n",
        "             max_action, \n",
        "             env.observation_space.low, \n",
        "             env.observation_space.high, \n",
        "             env.action_space.low, \n",
        "             env.action_space.high, \n",
        "             gauss_per_dim)\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "# logging variables:\n",
        "scores = []\n",
        "mean_scores = []\n",
        "last_scores = deque(maxlen=log_interval)\n",
        "distances = []\n",
        "mean_distances = []\n",
        "last_distance = deque(maxlen=log_interval)\n",
        "losses_mean_episode = []"
      ],
      "outputs": [],
      "execution_count": 64,
      "metadata": {
        "id": "IsvIgAee2qTB",
        "gather": {
          "logged": 1669830141371
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Eu7TQfmXu4CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure:\n",
        "for ep in range(start_episode + 1, max_episodes + 1):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    total_distance = 0\n",
        "    actor_losses = []\n",
        "    c1_losses = []\n",
        "    c2_losses = []\n",
        "\n",
        "    for t in range(max_timesteps):\n",
        "        # select action and add exploration noise:\n",
        "        action = policy.select_action(state)\n",
        "        action = action + np.random.normal(0, exploration_noise, size=action.shape[0])\n",
        "        action = action.clip(np.tile(env.action_space.low, num_steps), np.tile(env.action_space.high, num_steps))\n",
        "\n",
        "        # take action in env:\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "        state = next_state\n",
        "\n",
        "        total_reward += reward\n",
        "        if reward != -100:\n",
        "            total_distance += reward\n",
        "\n",
        "        # if episode is done then update policy:\n",
        "        if done or t == (max_timesteps - 1):\n",
        "            actor_loss, c1_loss, c2_loss = policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
        "            actor_losses.append(actor_loss)\n",
        "            c1_losses.append((c1_loss))\n",
        "            c2_losses.append(c2_loss)\n",
        "            break\n",
        "\n",
        "    mean_loss_actor = np.mean(actor_losses)\n",
        "    mean_loss_c1 = np.mean(c1_losses)\n",
        "    mean_loss_c2 = np.mean(c2_losses)\n",
        "    losses_mean_episode.append((ep, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
        "    print('\\rEpisode: {}/{},\\tScore: {:.2f},\\tDistance: {:.2f},\\tactor_loss: {:.2f},\\tc1_loss:{:.2f},\\tc2_loss:{:.2f}'\n",
        "        .format(ep, max_episodes,total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2), end=\"\")\n",
        "\n",
        "    # logging updates:\n",
        "    scores.append(total_reward)\n",
        "    distances.append(total_distance)\n",
        "    last_scores.append(total_reward)\n",
        "    last_distance.append(total_distance)\n",
        "    mean_score = np.mean(last_scores)\n",
        "    mean_distance = np.mean(last_distance)\n",
        "    FILE = 'record.dat'\n",
        "    data = [ep, total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
        "    \n",
        "    with open(FILE, \"ab\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    # print avg reward every log interval:\n",
        "    if ep % log_interval == 0:\n",
        "        policy.save(directory, filename, str(ep))\n",
        "        mean_scores.append(mean_score)\n",
        "        mean_distances.append(mean_distance)\n",
        "        print('\\rEpisode: {}/{},\\tMean Score: {:.2f},\\tMean Distance: {:.2f},\\tactor_loss: {:.2f},\\tc1_loss:{:.2f},\\tc2_loss:{:.2f}'\n",
        "            .format(ep, max_episodes, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
        "        \n",
        "        FILE = 'record_mean.dat'\n",
        "        data = [ep, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
        "        with open(FILE, \"ab\") as f:\n",
        "            pickle.dump(data, f)\n",
        "        \n",
        "        if mean_score >= 300 and len(last_scores) == 100:\n",
        "            print(\"Solved!\")\n",
        "            name = filename + '_solved'\n",
        "            policy.save(directory, name, str(ep))\n",
        "            break\n",
        "env.close()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100/1000,\tMean Score: -409.46,\tMean Distance: -320.46,\tactor_loss: 53.23,\tc1_loss:0.09,\tc2_loss:0.09\n",
            "Episode: 200/1000,\tMean Score: -352.21,\tMean Distance: -260.21,\tactor_loss: 79.34,\tc1_loss:0.32,\tc2_loss:0.30\n",
            "Episode: 300/1000,\tMean Score: -426.07,\tMean Distance: -338.07,\tactor_loss: 95.21,\tc1_loss:0.14,\tc2_loss:0.12\n",
            "Episode: 400/1000,\tMean Score: -269.20,\tMean Distance: -171.20,\tactor_loss: 100.37,\tc1_loss:0.11,\tc2_loss:0.09\n",
            "Episode: 500/1000,\tMean Score: -258.75,\tMean Distance: -160.75,\tactor_loss: 103.75,\tc1_loss:0.10,\tc2_loss:0.09\n",
            "Episode: 600/1000,\tMean Score: -352.26,\tMean Distance: -257.26,\tactor_loss: 107.29,\tc1_loss:0.22,\tc2_loss:0.19\n",
            "Episode: 688/1000,\tScore: -197.61,\tDistance: -97.61,\tactor_loss: 111.58,\tc1_loss:0.10,\tc2_loss:0.08"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqnqogd1cV8B",
        "outputId": "5d32b63d-9c31-42ae-a124-b4753ade4a62",
        "gather": {
          "logged": 1669848103760
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "aoRFdJH1u4CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [idx + 1 for idx in range(len(scores))]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, scores)\n",
        "plt.title('Scores [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(scorefigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "NXe3NShru4CD",
        "gather": {
          "logged": 1669848104344
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_reward = np.cumsum(scores)\n",
        "c_indices = np.arange(len(c_reward))\n",
        "rolling_avg = c_reward / c_indices\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, rolling_avg)\n",
        "plt.title('Average reward [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(cumfigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "y4tPHG0saO2D",
        "gather": {
          "logged": 1669848104392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(np.arange(len(distances)), distances)\n",
        "plt.title('Distances [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(distfigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hs-oS5UrE8sQ",
        "gather": {
          "logged": 1669848104439
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_distances = np.cumsum(distances)\n",
        "c_indices_distances = np.arange(len(c_distances))\n",
        "rolling_avg_distances = c_distances / c_indices_distances\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, rolling_avg_distances)\n",
        "plt.title('Mean distance [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(meandistfigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "a3NPuueUFRPI",
        "gather": {
          "logged": 1669848104474
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log"
      ],
      "metadata": {
        "id": "aHnAgjMGu4CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfname = datename + '_runlogs'\n",
        "df_avg_path = 'logs/' + env_name_clean + '_' + dfname + '_avg.csv'\n",
        "df_dist_path = 'logs/' + env_name_clean + '_' + dfname + '_distances.csv'\n",
        "\n",
        "df_avg = pd.DataFrame({'avg_score': rolling_avg})\n",
        "df_dist = pd.DataFrame({'distances': distances})\n",
        "\n",
        "df_avg.to_csv(df_avg_path, sep = ';', header = True, index = False)\n",
        "df_dist.to_csv(df_dist_path, sep = ';', header = True, index = False)\n",
        "\n",
        "print(\"Done logging results\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UU1BATVPu4CF",
        "gather": {
          "logged": 1669848104530
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "76pGlcaiu4CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compscores = 0\n",
        "for i in range(n_rounds):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        env.render()\n",
        "        \n",
        "        action = policy.select_action(state)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "        \n",
        "        state = next_state\n",
        "        score += reward\n",
        "    \n",
        "    compscores += score\n",
        "    print(\"Competitive round \", i + 1, \" Overall score \", compscores)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vjCoTRAtu4CI",
        "gather": {
          "logged": 1669848104588
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score"
      ],
      "metadata": {
        "id": "Lzg3fZnYu4CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(outfilename, \"w\") as f:\n",
        "    f.writelines(\"%s: %i\\n\" % (env_name_clean, compscores))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kdr_4ZppOJs3",
        "gather": {
          "logged": 1669848104646
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(mode == 'colab'):\n",
        "    from google.colab import files\n",
        "    files.download(model_file)\n",
        "    files.download(model_file_target)\n",
        "    files.download(outfilename)\n",
        "    files.download(df_avg_path)\n",
        "    files.download(df_dist_path)\n",
        "    files.download('images/' + scorefigname)\n",
        "    files.download('images/' + cumfigname)\n",
        "    files.download('images/' + distfigname)\n",
        "    files.download('images/' + meandistfigname)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SqaiDFmKOJWg",
        "gather": {
          "logged": 1669848104707
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}