{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install Box2D\n",
        "!pip install gym[Box2D]\n",
        "!pip install snntorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azKtoHK9kqBb",
        "outputId": "a1619caf-49be-47d9-d0d5-1ae1cc8849f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym) (3.9.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[Box2D] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box2D]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box2D]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box2D]) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box2D]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d_py-2.3.5-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 67 kB/s \n",
            "\u001b[?25hCollecting swig==4.*\n",
            "  Downloading swig-4.0.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box2D]) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box2D]) (4.1.1)\n",
            "Installing collected packages: swig, pygame, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting snntorch\n",
            "  Downloading snntorch-0.5.3-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from snntorch) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->snntorch) (4.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->snntorch) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->snntorch) (2022.5)\n",
            "Installing collected packages: snntorch\n",
            "Successfully installed snntorch-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\n",
        "import time\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import surrogate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "\n",
        "import gym\n",
        "from SNNreinforcement import Agent\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from Producer import Producer\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "xPkTukZfki-l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "19qgKET6kB6G"
      },
      "outputs": [],
      "source": [
        "class Producer:\n",
        "    def __init__(self):\n",
        "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        self.sock.bind((\"localhost\", 34567))\n",
        "        self.sock.listen(5)\n",
        "        (self.conn, address) = self.sock.accept()\n",
        "\n",
        "    def send(self, dataMat):\n",
        "        self.conn.send(dataMat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, beta=0.95):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        spike_grad1 = surrogate.fast_sigmoid()\n",
        "        spike_grad2 = surrogate.fast_sigmoid()\n",
        "        spike_grad3 = surrogate.fast_sigmoid()\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.lif1 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad1)\n",
        "        print(self.lif1)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
        "        self.lif2 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad2)\n",
        "        self.fc3 = nn.Linear(num_hidden, num_outputs)\n",
        "        self.lif3 = snn.Leaky(beta=beta, learn_beta=True, threshold=1e5, reset_mechanism=\"none\", spike_grad=spike_grad3)\n",
        "\n",
        "    def _convert_to_spikes(self, data):\n",
        "        return snn.spikegen.delta(data, threshold=0.1, padding=False, off_spike=True)\n",
        "\n",
        "    def createGauss(self, mins, maxes, numPerDim, amplMax, dims):\n",
        "        self.amplMax = amplMax\n",
        "        self.numPerDim = numPerDim\n",
        "        self.M = []\n",
        "        self.sigma = []\n",
        "        for i in range(dims):\n",
        "            M, sigma = np.linspace(mins[i], maxes[i], numPerDim, retstep=True)\n",
        "            self.M.append(M)\n",
        "            self.sigma += [sigma, ] * self.numPerDim\n",
        "        self.M = torch.tensor(np.array(self.M).reshape(-1, self.numPerDim), dtype=torch.float).cuda()\n",
        "        self.sigma = torch.tensor(np.array(self.sigma).reshape(-1, self.numPerDim), dtype=torch.float).cuda()\n",
        "\n",
        "    def gaussianCurrents(self, data):\n",
        "        x = data.unsqueeze(-1).repeat([1, 1, self.numPerDim])\n",
        "        return (torch.exp(-1 / 2 * ((x - self.M) / self.sigma) ** 2) * self.amplMax).reshape(data.shape[0], -1)\n",
        "\n",
        "    def forward(self, x, num_steps=16):\n",
        "\n",
        "        x = self.gaussianCurrents(x)\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        spk1_rec = []\n",
        "        mem1_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1, )\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "            spk1_rec.append(spk1)\n",
        "            mem1_rec.append(mem1)\n",
        "\n",
        "\n",
        "        return x, [torch.stack(spk1_rec, dim=0), torch.stack(spk2_rec, dim=0), torch.stack(spk3_rec, dim=0),\n",
        "                torch.stack(mem1_rec, dim=0), torch.stack(mem2_rec, dim=0), torch.stack(mem3_rec, dim=0)], \\\n",
        "               torch.stack(mem3_rec, dim=0)"
      ],
      "metadata": {
        "id": "EPPMCOJhklda"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, lr, gamma, mem_size, n_actions, epsilon, batch_size, input_dims, epsilon_dec=0.99988,\n",
        "                 epsilon_end=0.01, targetUpdateSteps=20, q_dir='tmp\\\\q', visualize=False):\n",
        "        self.visualize= visualize\n",
        "        self.targetUpdateSteps = targetUpdateSteps\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.mem_size = mem_size\n",
        "        self.mem_cntr = 0\n",
        "        self.stepnum = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_dec = epsilon_dec\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.low = np.array(\n",
        "            [\n",
        "                # these are bounds for position\n",
        "                # realistically the environment should have ended\n",
        "                # long before we reach more than 50% outside\n",
        "                -1.5,\n",
        "                -1.5,\n",
        "                # velocity bounds is 5x rated speed\n",
        "                -5.0,\n",
        "                -5.0,\n",
        "                -np.pi,\n",
        "                -5.0,\n",
        "                -0.0,\n",
        "                -0.0,\n",
        "            ]\n",
        "        ).astype(np.float32)\n",
        "        self.high = np.array(\n",
        "            [\n",
        "                # these are bounds for position\n",
        "                # realistically the environment should have ended\n",
        "                # long before we reach more than 50% outside\n",
        "                1.5,\n",
        "                1.5,\n",
        "                # velocity bounds is 5x rated speed\n",
        "                5.0,\n",
        "                5.0,\n",
        "                np.pi,\n",
        "                5.0,\n",
        "                1.0,\n",
        "                1.0,\n",
        "            ]\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        self.dtype = torch.float\n",
        "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        self.device = torch.device(\"cpu\")\n",
        "\n",
        "        gaussPerDim = 64\n",
        "        hiddenSize = 128\n",
        "        self.q_net = Net(input_dims[0] * gaussPerDim, hiddenSize, n_actions).to(self.device)\n",
        "        self.q_net.createGauss(self.low, self.high, gaussPerDim, 1.0, input_dims[0])\n",
        "        self.t_net = Net(input_dims[0] * gaussPerDim, hiddenSize, n_actions).to(self.device)\n",
        "        self.t_net.createGauss(self.low, self.high, gaussPerDim, 1.0, input_dims[0])\n",
        "        self.t_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        # self.loss = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "        # self.q_eval = DeepQNetwork(lr, n_actions, input_dims=input_dims, name='q_eval', chkpt_dir=q_dir)\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_dims))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_dims))\n",
        "        self.action_memory = np.zeros((self.mem_size, self.n_actions), dtype=np.int8)\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.int8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, terminal):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.reward_memory[index] = reward\n",
        "        actions = np.zeros(self.n_actions)\n",
        "        actions[action] = 1.0\n",
        "        self.action_memory[index] = actions\n",
        "        self.terminal_memory[index] = 1 - terminal\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state = state[np.newaxis, :]\n",
        "        rand = np.random.random()\n",
        "        # print(self.epsilon)\n",
        "        if rand < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.q_net.eval()\n",
        "                gaussX, spikes, actions = self.q_net(torch.tensor(state, dtype=self.dtype).to(self.device))\n",
        "                actions = actions.cpu()[-1, 0, :].detach().numpy()\n",
        "                action = np.argmax(actions)\n",
        "        return action, [s.cpu().numpy() for s in spikes], gaussX.cpu().numpy()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.mem_cntr > self.batch_size:\n",
        "            max_mem = self.mem_cntr if self.mem_cntr < self.mem_size else self.mem_size\n",
        "\n",
        "            batch = np.random.choice(max_mem, self.batch_size)\n",
        "            state_batch = self.state_memory[batch]\n",
        "            new_state_batch = self.new_state_memory[batch]\n",
        "            action_batch = self.action_memory[batch]\n",
        "            action_values = np.array(self.action_space, dtype=np.int8)\n",
        "            action_indices = np.dot(action_batch, action_values)\n",
        "            reward_batch = self.reward_memory[batch]\n",
        "            terminal_batch = self.terminal_memory[batch]\n",
        "\n",
        "            self.q_net.train()\n",
        "            evalspikes, q_eval = self.q_net.forward(torch.tensor(state_batch, dtype=self.dtype).to(self.device))\n",
        "            q_eval = q_eval[-1, :, :]\n",
        "\n",
        "            self.t_net.eval()\n",
        "            _, q_next = self.t_net.forward(torch.tensor(new_state_batch, dtype=self.dtype).to(self.device))\n",
        "\n",
        "            q_next = q_next[-1, :, :]\n",
        "\n",
        "            q_target = torch.clone(q_eval).detach()\n",
        "\n",
        "\n",
        "            batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "            q_target[batch_index, action_indices] = (\n",
        "                    torch.tensor(reward_batch, device=self.device) + torch.tensor(self.gamma, device=self.device) \\\n",
        "                    * torch.max(q_next, dim=1).values * torch.tensor(terminal_batch, device=self.device)).type(\n",
        "                self.dtype).detach()\n",
        "\n",
        "            criterion = nn.MSELoss()\n",
        "            loss = criterion(q_eval, q_target)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            for param in self.q_net.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad.data.clamp_(-1, 1)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.epsilon = self.epsilon * self.epsilon_dec if self.epsilon > self.epsilon_end else self.epsilon_end\n",
        "\n",
        "            if self.stepnum % self.targetUpdateSteps == 0:\n",
        "                self.t_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "            self.stepnum += 1\n",
        "\n",
        "    def save_models(self):\n",
        "        torch.save(self.q_net.state_dict(), \"./tmp/dict9 LIF\")\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_net.load_state_dict(torch.load(\"./tmp/dict9 LIF\"))\n",
        "        self.q_net.eval()"
      ],
      "metadata": {
        "id": "hFR8ofWcmPl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # np.random.seed(1233)\n",
        "\n",
        "    stime = time.time()\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    # env.seed(42)\n",
        "    lr = 0.0005\n",
        "    n_games = 500\n",
        "    agent = Agent(gamma=0.99, epsilon=0.1, lr=lr, input_dims=[8], n_actions=4, mem_size=1000000, batch_size=64, epsilon_end=0.01)\n",
        "    # '''\n",
        "    filename = \"lunarlander.png\"\n",
        "    scores = []\n",
        "    eps_history = []\n",
        "\n",
        "    score = 0\n",
        "    agent.load_models()\n",
        "\n",
        "    state = agent.q_net.state_dict()\n",
        "    state[\"lif1.beta\"] = torch.tensor(0.95,dtype=torch.float).cuda()\n",
        "    state[\"lif2.beta\"] = torch.tensor(0.95, dtype=torch.float).cuda()\n",
        "    state[\"lif3.beta\"] = torch.tensor(1.0, dtype=torch.float).cuda()\n",
        "    agent.q_net.load_state_dict(state)\n",
        "    print(agent.q_net.lif1.beta)\n",
        "\n",
        "\n",
        "    i = -1\n",
        "    while time.time() - stime < 60 * 240:\n",
        "        i += 1\n",
        "        done = False\n",
        "        if i % 10 == 0 and i > 0:\n",
        "            avg_score = np.mean(scores[max(0, i - 10):(i + 1)])\n",
        "            print('episode', i, 'score', score, 'average_score %.3f' % avg_score, 'epsilon %.3f' % agent.epsilon)\n",
        "            agent.save_models()\n",
        "        else:\n",
        "            print('episode', i, 'score', score)\n",
        "\n",
        "        observation = env.reset()\n",
        "        score = 0\n",
        "        while not done:\n",
        "            if i % 5 == 0:\n",
        "                env.render()\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            # print(score, reward)\n",
        "            agent.store_transition(observation, action, reward, observation_, int(done))\n",
        "            observation = observation_\n",
        "\n",
        "            agent.learn()\n",
        "\n",
        "        scores.append(score)\n",
        "        eps_history.append(agent.epsilon)\n",
        "\n",
        "    x = [idx + 1 for idx in range(i + 1)]\n",
        "    plt.figure(0)\n",
        "    plt.plot(x, scores)\n",
        "    plt.grid()\n",
        "    plt.figure(1)\n",
        "    plt.plot(x, eps_history)\n",
        "    plt.grid()\n",
        "    print(\"Total time: \", time.time() - stime)\n",
        "    plt.show()\n",
        "    # '''\n",
        "    input(\"Press Enter to start trials\\n\")\n",
        "\n",
        "    compscores = 0\n",
        "\n",
        "    # agent.load_models()\n",
        "    print(agent.epsilon)\n",
        "    for i in range(5):\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        score = 0\n",
        "        while not done:\n",
        "            env.render()\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            agent.store_transition(observation, action, reward, observation_, int(done))\n",
        "            observation = observation_\n",
        "            score += reward\n",
        "        compscores += score\n",
        "        print(\"Competitive round \", i + 1, \" Overall score \", compscores)\n",
        "\n",
        "    with open(\"scoreboard.txt\", \"w\") as f:\n",
        "        f.writelines(\"%s: %i\\n\" % (\"MIkro Lander\", compscores))\n",
        "    input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "8C7FWnDwlQ-O",
        "outputId": "cd766b87-784c-40aa-ca60-4317c0a2d846"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leaky()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6a406eb2ebc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lunarlander.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SNNreinforcement.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lr, gamma, mem_size, n_actions, epsilon, batch_size, input_dims, epsilon_dec, epsilon_end, targetUpdateSteps, q_dir, visualize)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mhiddenSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgaussPerDim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddenSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateGauss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussPerDim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgaussPerDim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddenSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateGauss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussPerDim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SNNreinforcement.py\u001b[0m in \u001b[0;36mcreateGauss\u001b[0;34m(self, mins, maxes, numPerDim, amplMax, dims)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumPerDim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumPerDim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumPerDim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prod = Producer()\n",
        "np.random.seed(42)\n",
        "env=gym.make('LunarLander-v2')\n",
        "env.seed(10)\n",
        "lr=0.0005\n",
        "agent=Agent(gamma=0.99, epsilon=0.0, lr=lr, input_dims=[8],n_actions=4, mem_size=1000000, batch_size=64, visualize=True)\n",
        "\n",
        "agent.load_models()\n",
        "\n",
        "compscores=0\n",
        "\n",
        "agent.load_models()\n",
        "print(agent.epsilon)\n",
        "i=0\n",
        "while True:\n",
        "    done=False\n",
        "    observation=env.reset()\n",
        "    score=0\n",
        "    while not done:\n",
        "        time.sleep(0.1)\n",
        "        img = env.render(mode='rgb_array')\n",
        "        action, data, gaussX = agent.choose_action(observation)\n",
        "\n",
        "        prod.send(pickle.dumps([data,gaussX.reshape(8,64), img]))\n",
        "\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        agent.store_transition(observation, action, reward, observation_, int(done))\n",
        "        observation = observation_\n",
        "        score += reward\n",
        "    if i<5:\n",
        "        compscores+=score\n",
        "        print(\"Competitive round \",i+1,\" Overall score \",compscores)\n",
        "\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "bg442JVzmjdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}