{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Directories"
      ],
      "metadata": {
        "id": "l8Pk2GDiu4Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "mode = -1\n",
        "mode = 'local'\n",
        "# Install necessary files and create necessary folders (Only when ran in Google Drive)\n",
        "!pip install snntorch==0.5.3\n",
        "!pip install gym==0.25.2\n",
        "!pip install box2d-py==2.3.5\n",
        "!pip install torch==1.12.1+cu113\n",
        "!pip install \n",
        "!pip install swig\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "#!pip install gym[box2d]\n",
        "!mkdir tmp\n",
        "!mkdir logs\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#!cp \"/content/drive/MyDrive/Colab Notebooks/dict9 LIF\" \"/content/tmp/\"\n",
        "mode = 'colab'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: snntorch==0.5.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.5.3)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from snntorch==0.5.3) (1.21.6)\nRequirement already satisfied: torch>=1.1.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from snntorch==0.5.3) (1.12.0)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from snntorch==0.5.3) (1.1.5)\nRequirement already satisfied: matplotlib in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from snntorch==0.5.3) (3.2.1)\nRequirement already satisfied: typing-extensions in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from torch>=1.1.0->snntorch==0.5.3) (4.4.0)\nRequirement already satisfied: pytz>=2017.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas->snntorch==0.5.3) (2022.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from pandas->snntorch==0.5.3) (2.8.2)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from matplotlib->snntorch==0.5.3) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from matplotlib->snntorch==0.5.3) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from matplotlib->snntorch==0.5.3) (1.4.4)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->snntorch==0.5.3) (1.16.0)\nRequirement already satisfied: gym==0.25.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.25.2)\nRequirement already satisfied: cloudpickle>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym==0.25.2) (1.6.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym==0.25.2) (0.0.8)\nRequirement already satisfied: numpy>=1.18.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym==0.25.2) (1.21.6)\nRequirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym==0.25.2) (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gym==0.25.2) (3.9.0)\nRequirement already satisfied: box2d-py==2.3.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (2.3.5)\n\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.12.1+cu113 (from versions: 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0)\u001b[0m\n\u001b[31mERROR: No matching distribution found for torch==1.12.1+cu113\u001b[0m\n\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\nRequirement already satisfied: swig in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: gym in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.25.2)\nRequirement already satisfied: numpy>=1.18.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym) (1.21.6)\nRequirement already satisfied: cloudpickle>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym) (1.6.0)\nRequirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym) (4.13.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym) (0.0.8)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gym) (3.9.0)\nRequirement already satisfied: gym[box2d] in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (0.25.2)\nRequirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym[box2d]) (4.13.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym[box2d]) (0.0.8)\nRequirement already satisfied: cloudpickle>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym[box2d]) (1.6.0)\nRequirement already satisfied: numpy>=1.18.0 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym[box2d]) (1.21.6)\nRequirement already satisfied: swig==4.*; extra == \"box2d\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym[box2d]) (4.1.0)\nRequirement already satisfied: box2d-py==2.3.5; extra == \"box2d\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym[box2d]) (2.3.5)\nRequirement already satisfied: pygame==2.1.0; extra == \"box2d\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from gym[box2d]) (2.1.0)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gym[box2d]) (3.9.0)\nmkdir: cannot create directory ‘tmp’: File exists\nmkdir: cannot create directory ‘logs’: File exists\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EyOhCiEu4Bk",
        "outputId": "a79af4f7-7ac2-48b3-89e1-d54288242797",
        "gather": {
          "logged": 1670256493655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "hv0oAX5su4Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "from datetime import datetime\n",
        "from collections import deque, namedtuple\n",
        "from gym import wrappers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.cuda.memory import mem_get_info\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "from snntorch import surrogate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "YaOc25LDu4Br",
        "gather": {
          "logged": 1670256495053
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A small method to save plots\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format='png', dpi=resolution)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "1W8CBfWbvZM8",
        "gather": {
          "logged": 1670256495397
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters ###"
      ],
      "metadata": {
        "id": "DtnGruUvu4Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 5e-4                   # Learning rate # lr = 0.001\n",
        "gauss_per_dim = 64\n",
        "max_episodes = 1000         # max num of episodes\n",
        "max_timesteps = 2000        # max timesteps in one episode\n",
        "num_steps = 16              # How many steps to loop in prediction\n",
        "n_rounds = 5                # Number of competitive rounds to play\n",
        "log_interval = 100          # print avg reward after interval\n",
        "random_seed = 0\n",
        "gamma = 0.99                # discount for future rewards\n",
        "batch_size = 128            # num of transitions sampled from replay buffer\n",
        "exploration_noise = 0.1\n",
        "polyak = 0.995              # target policy update parameter (1-tau)\n",
        "policy_noise = 0.2          # target policy smoothing noise\n",
        "noise_clip = 0.5\n",
        "policy_delay = 2            # delayed policy updates parameter\n",
        "start_episode = 0"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "uSJzX05Vu4B1",
        "gather": {
          "logged": 1670256495738
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'BipedalWalker-v3'\n",
        "env_name_clean = env_name.lower().replace('-','_')\n",
        "filename = env_name_clean + \".png\"\n",
        "outfilename = env_name_clean + \"scoreboard.txt\"\n",
        "datename = str(datetime.now()).split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_') \n",
        "scorefigname = env_name_clean + '_' + datename + '_scores.png'\n",
        "epsfigname = env_name_clean + '_' + datename + '_eps_history.png'\n",
        "cumfigname = env_name_clean + '_' + datename + '_cumulated_reward.png'\n",
        "distfigname = env_name_clean + '_' + datename + '_distances.png'\n",
        "meandistfigname = env_name_clean + '_' + datename + '_mean_distances.png'\n",
        "model_file = './tmp/'  + env_name_clean + '_dict9'\n",
        "model_file_target = model_file + '_target'\n",
        "directory = \"./tmp/\"\n",
        "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Env name (clean):',env_name_clean)\n",
        "print('Datetime (clean):', datename)\n",
        "print('Output file:', outfilename)\n",
        "print('Model file:', model_file)\n",
        "print('Running training on: ', device)\n",
        "print('Environment:', mode)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Env name (clean): bipedalwalker_v3\nDatetime (clean): 2022_12_05_16_08_15\nOutput file: bipedalwalker_v3scoreboard.txt\nModel file: ./tmp/bipedalwalker_v3_dict9\nRunning training on:  cuda\nEnvironment: colab\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAqS6F-Yu4By",
        "outputId": "da0856fb-46a2-4e66-f76f-e9c876551708",
        "gather": {
          "logged": 1670256496024
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes"
      ],
      "metadata": {
        "id": "wU6d2CLFu4B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor"
      ],
      "metadata": {
        "id": "dlbfSeXDSIiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 480)\n",
        "        self.l2 = nn.Linear(480, 240)\n",
        "        self.l3 = nn.Linear(240, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        a = torch.tanh(self.l3(a)) * self.max_action\n",
        "        return a"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "P_Kt1Ienp0dx",
        "gather": {
          "logged": 1670256496347
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Critic"
      ],
      "metadata": {
        "id": "J0nnEGXw8YHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, beta=0.95):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Initialize grad objects        \n",
        "        spike_grad1 = surrogate.fast_sigmoid()\n",
        "        spike_grad2 = surrogate.fast_sigmoid()\n",
        "        spike_grad3 = surrogate.fast_sigmoid()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.lif1 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad1)\n",
        "        \n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.lif2 = snn.Leaky(beta=beta, learn_threshold=False, spike_grad=spike_grad2)\n",
        "\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "        self.lif3 = snn.Leaky(beta=beta, learn_beta=True, threshold=1e5, reset_mechanism=\"none\", spike_grad=spike_grad3)\n",
        "\n",
        "    def _convert_to_spikes(self, data):\n",
        "        return snn.spikegen.delta(data, threshold=0.1, padding=False, off_spike=True)\n",
        "\n",
        "    def createGauss(self, mins, maxes, numPerDim, amplMax, dims):\n",
        "        self.amplMax = amplMax\n",
        "        self.numPerDim = numPerDim\n",
        "        self.M = []\n",
        "        self.sigma = []\n",
        "        for i in range(dims):\n",
        "            M, sigma = np.linspace(mins[i], maxes[i], numPerDim, retstep=True)\n",
        "            self.M.append(M)\n",
        "            self.sigma += [sigma, ] * self.numPerDim\n",
        "        self.M = torch.tensor(np.array(self.M).reshape(-1, self.numPerDim), dtype=torch.float).to(device)\n",
        "        self.sigma = torch.tensor(np.array(self.sigma).reshape(-1, self.numPerDim), dtype=torch.float).to(device)\n",
        "\n",
        "    def gaussianCurrents(self, data):\n",
        "        x = data.unsqueeze(-1).repeat([1, 1, self.numPerDim])\n",
        "        return (torch.exp(-1 / 2 * ((x - self.M) / self.sigma) ** 2) * self.amplMax).reshape(data.shape[0], -1)\n",
        "\n",
        "    def forward(self, state, action, num_steps = 16):\n",
        "        x = torch.cat([state, action], 1)\n",
        "        x = self.gaussianCurrents(x)\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer#\n",
        "        spk1_rec = []\n",
        "        mem1_rec = []\n",
        "        \n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "        \n",
        "        spk3_rec = []\n",
        "        mem3_rec = []        \n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            \n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            \n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            spk1_rec.append(spk1)\n",
        "            mem1_rec.append(mem1)\n",
        "            \n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "            \n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return mem3_rec[num_steps-1]"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "9j2tGXDTqkXA",
        "gather": {
          "logged": 1670256496661
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReplayBuffer"
      ],
      "metadata": {
        "id": "eywnQwfISDqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=5e5):\n",
        "        self.buffer = []\n",
        "        self.max_size = int(max_size)\n",
        "        self.size = 0\n",
        "    \n",
        "    def add(self, transition):\n",
        "        self.size +=1\n",
        "        # transiton is tuple of (state, action, reward, next_state, done)\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # delete 1/5th of the buffer when full\n",
        "        if self.size > self.max_size:\n",
        "            del self.buffer[0:int(self.size/5)]\n",
        "            self.size = len(self.buffer)\n",
        "        \n",
        "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "        state, action, reward, next_state, done = [], [], [], [], []\n",
        "        \n",
        "        for i in indexes:\n",
        "            s, a, r, s_, d = self.buffer[i]\n",
        "            state.append(np.array(s, copy=False))\n",
        "            action.append(np.array(a, copy=False))\n",
        "            reward.append(np.array(r, copy=False))\n",
        "            next_state.append(np.array(s_, copy=False))\n",
        "            done.append(np.array(d, copy=False))\n",
        "        \n",
        "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "izTUGxP5QsoG",
        "gather": {
          "logged": 1670256497014
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "W4KED1jNSLIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3:\n",
        "    def __init__(self, lr, state_size, action_size, max_action, low, high, action_low, action_high, gauss_per_dim):\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "\n",
        "        critic_low = np.concatenate([low, action_low])         \n",
        "        critic_high = np.concatenate([high, action_high])\n",
        "        critic_input = (state_size + action_size) * gauss_per_dim\n",
        "\n",
        "        self.critic_1 = Critic(critic_input).to(device)\n",
        "        self.critic_1_target = Critic(critic_input).to(device)\n",
        "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
        "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
        "        \n",
        "        self.critic_1.createGauss(critic_low, critic_high, gauss_per_dim, 1.0, state_size+action_size)\n",
        "        self.critic_1_target.createGauss(critic_low, critic_high, gauss_per_dim, 1.0, state_size+action_size)\n",
        "\n",
        "        self.critic_2 = Critic(critic_input).to(device)\n",
        "        self.critic_2_target = Critic(critic_input).to(device)\n",
        "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
        "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
        "\n",
        "        self.critic_2.createGauss(critic_low, critic_high, gauss_per_dim, 1.0, state_size+action_size)\n",
        "        self.critic_2_target.createGauss(critic_low, critic_high, gauss_per_dim, 1.0, state_size+action_size)\n",
        "\n",
        "        self.max_action = max_action\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "    \n",
        "    def update(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
        "        for i in range(n_iter):\n",
        "            # Sample a batch of transitions from replay buffer:\n",
        "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.FloatTensor(action_).to(device)\n",
        "            reward = torch.FloatTensor(reward).reshape((batch_size, 1)).to(device)\n",
        "            next_state = torch.FloatTensor(next_state).to(device)\n",
        "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
        "            \n",
        "            # Select next action according to target policy:\n",
        "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            actor_target_out = self.actor_target(next_state)\n",
        "            noise = torch.reshape(noise, tuple(actor_target_out.shape))\n",
        "            next_action = (actor_target_out + noise)\n",
        "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute target Q-value:\n",
        "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
        "            \n",
        "            # Optimize Critic 1:\n",
        "            current_Q1 = self.critic_1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "            self.critic_1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic_1_optimizer.step()\n",
        "            \n",
        "            # Optimize Critic 2:\n",
        "            current_Q2 = self.critic_2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "            self.critic_2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic_2_optimizer.step()\n",
        "            \n",
        "            # Delayed policy updates:\n",
        "            if i % policy_delay == 0:\n",
        "                # Compute actor loss:\n",
        "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
        "                \n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "                \n",
        "                # Polyak averaging update:\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "                \n",
        "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
        "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
        "\n",
        "        return actor_loss.cpu().data.numpy(), loss_Q1.cpu().data.numpy(), loss_Q2.cpu().data.numpy()\n",
        "                \n",
        "    def save(self, directory, name, ep):\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor_ep%s.pth' % (directory, name, ep))\n",
        "        torch.save(self.actor_target.state_dict(), '%s/%s_actor_target_ep%s.pth' % (directory, name, ep))\n",
        "        \n",
        "        torch.save(self.critic_1.state_dict(), '%s/%s_crtic_1_ep%s.pth' % (directory, name, ep))\n",
        "        torch.save(self.critic_1_target.state_dict(), '%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep))\n",
        "        \n",
        "        torch.save(self.critic_2.state_dict(), '%s/%s_crtic_2_ep%s.pth' % (directory, name, ep))\n",
        "        torch.save(self.critic_2_target.state_dict(), '%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep))\n",
        "        \n",
        "    def load(self, directory, name, ep):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_1.load_state_dict(torch.load('%s/%s_crtic_1_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.critic_1_target.load_state_dict(torch.load('%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        self.critic_2.load_state_dict(torch.load('%s/%s_crtic_2_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.critic_2_target.load_state_dict(torch.load('%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        \n",
        "    def load_actor(self, directory, name, ep):\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
        "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "-QRt0zUKgmQB",
        "gather": {
          "logged": 1670256497309
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "g6VdDDb3NXPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gym.logger.set_level(40)\n",
        "env_name = \"BipedalWalker-v3\"\n",
        "# env_name = \"MountainCarContinuous-v0\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "policy = TD3(lr, \n",
        "             state_dim, \n",
        "             action_dim, \n",
        "             max_action, \n",
        "             env.observation_space.low, \n",
        "             env.observation_space.high, \n",
        "             env.action_space.low, \n",
        "             env.action_space.high, \n",
        "             gauss_per_dim)\n",
        "\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "# logging variables:\n",
        "scores = []\n",
        "mean_scores = []\n",
        "last_scores = deque(maxlen=log_interval)\n",
        "distances = []\n",
        "mean_distances = []\n",
        "last_distance = deque(maxlen=log_interval)\n",
        "losses_mean_episode = []"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "IsvIgAee2qTB",
        "gather": {
          "logged": 1670256498513
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Eu7TQfmXu4CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure:\n",
        "for ep in range(start_episode + 1, max_episodes + 1):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    total_distance = 0\n",
        "    actor_losses = []\n",
        "    c1_losses = []\n",
        "    c2_losses = []\n",
        "\n",
        "    for t in range(max_timesteps):\n",
        "        # select action and add exploration noise:\n",
        "        action = policy.select_action(state)\n",
        "        action = action + np.random.normal(0, exploration_noise, size=action.shape[0])\n",
        "        action = action.clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "        # take action in env:\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "        state = next_state\n",
        "\n",
        "        total_reward += reward\n",
        "        if reward != -100:\n",
        "            total_distance += reward\n",
        "\n",
        "        # if episode is done then update policy:\n",
        "        if done or t == (max_timesteps - 1):\n",
        "            actor_loss, c1_loss, c2_loss = policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
        "            actor_losses.append(actor_loss)\n",
        "            c1_losses.append((c1_loss))\n",
        "            c2_losses.append(c2_loss)\n",
        "            break\n",
        "\n",
        "    mean_loss_actor = np.mean(actor_losses)\n",
        "    mean_loss_c1 = np.mean(c1_losses)\n",
        "    mean_loss_c2 = np.mean(c2_losses)\n",
        "    losses_mean_episode.append((ep, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
        "    print('\\rEpisode: {}/{},\\tScore: {:.2f},\\tDistance: {:.2f},\\tactor_loss: {:.2f},\\tc1_loss:{:.2f},\\tc2_loss:{:.2f}'\n",
        "        .format(ep, max_episodes,total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2), end=\"\")\n",
        "\n",
        "    # logging updates:\n",
        "    scores.append(total_reward)\n",
        "    distances.append(total_distance)\n",
        "    last_scores.append(total_reward)\n",
        "    last_distance.append(total_distance)\n",
        "    mean_score = np.mean(last_scores)\n",
        "    mean_distance = np.mean(last_distance)\n",
        "    FILE = 'record.dat'\n",
        "    data = [ep, total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
        "    \n",
        "    with open(FILE, \"ab\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    # print avg reward every log interval:\n",
        "    if ep % log_interval == 0:\n",
        "        policy.save(directory, filename, str(ep))\n",
        "        mean_scores.append(mean_score)\n",
        "        mean_distances.append(mean_distance)\n",
        "        print('\\rEpisode: {}/{},\\tMean Score: {:.2f},\\tMean Distance: {:.2f},\\tactor_loss: {:.2f},\\tc1_loss:{:.2f},\\tc2_loss:{:.2f}'\n",
        "            .format(ep, max_episodes, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
        "        \n",
        "        FILE = 'record_mean.dat'\n",
        "        data = [ep, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
        "        with open(FILE, \"ab\") as f:\n",
        "            pickle.dump(data, f)\n",
        "        \n",
        "        if mean_score >= 300 and len(last_scores) == 100:\n",
        "            print(\"Solved!\")\n",
        "            name = filename + '_solved'\n",
        "            policy.save(directory, name, str(ep))\n",
        "            break\n",
        "env.close()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Episode: 100/1000,\tMean Score: -100.04,\tMean Distance: -7.04,\tactor_loss: 54.00,\tc1_loss:2.92,\tc2_loss:4.70\nEpisode: 200/1000,\tMean Score: -96.80,\tMean Distance: -5.80,\tactor_loss: 55.72,\tc1_loss:0.38,\tc2_loss:0.48\nEpisode: 300/1000,\tMean Score: -94.80,\tMean Distance: -39.80,\tactor_loss: 21.58,\tc1_loss:0.31,\tc2_loss:0.36\nEpisode: 400/1000,\tMean Score: -61.43,\tMean Distance: -60.43,\tactor_loss: 11.26,\tc1_loss:0.44,\tc2_loss:0.30\nEpisode: 451/1000,\tScore: -30.08,\tDistance: -30.08,\tactor_loss: 11.16,\tc1_loss:0.98,\tc2_loss:0.98"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Bad pipe message: %s [b\"\\x14\\xeb\\x1b\\x0c[\\x83'k\\xf8*Ux\\xf0\\xa9@s;\\xe5 G\\\\\\xd8v\\x12\\x13\\xca\\xb32\\xebg\\x9a\\x8c\\xc4T\\x05\\x85\\xec\\xc8\\xe1\\xd4\\xb0\\xdbo\\x1e\\xd6\\xd5\\x82\\xcd]\\x06\\x07\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\"]\nBad pipe message: %s [b\"\\x03\\xd7%\\x14\\x80\\xb8\\xf8\\xc1m\\xf9\\xae\\xd4\\xa3\\xc1E\\xfcl!\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\"]\nBad pipe message: %s [b'\\x03\\x03\\x02\\x03\\x03', b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'\\x8e\\xa8#\\xa5\\x80\\xebPV\\x04\\x8f\\xe4O\\\\G_1\\xa2\\xfb\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa']\nBad pipe message: %s [b\"\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\", b'\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07']\nBad pipe message: %s [b'\\x08\\t\\x08\\n\\x08\\x0b\\x08']\nBad pipe message: %s [b'\\x05\\x08\\x06']\nBad pipe message: %s [b'\\x05\\x01\\x06', b'', b'\\x03\\x03']\nBad pipe message: %s [b'G\\xcd\\x98\\xe7<H\\xe4\\xbe\\x9dP\\xbf\\xe4\\xc3\\xf3E+\\xac,\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01']\nBad pipe message: %s [b'']\nBad pipe message: %s [b'', b'\\x02']\nBad pipe message: %s [b'\\x05\\x02\\x06']\nBad pipe message: %s [b'EU\\x8df\\xa4\\x97\\xf5t\\x18\"s\\xb4@rV~\\xbd\\r\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03']\nBad pipe message: %s [b'\\x06\\xb4\\xaf9\\xa3\\xb0\\x8a\\xb5\\xb9\\x81\\xf9\\xac\\xd55\\xa3f\\xd3\\xfe\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x00']\nBad pipe message: %s [b\"/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00\"]\nBad pipe message: %s [b'\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0']\nBad pipe message: %s [b'\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00', b' \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05']\nBad pipe message: %s [b'\\x03', b'\\x04\\x02\\x04', b'\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\nBad pipe message: %s [b\"\\x9d\\x9e(0\\xea\\x8dv\\xba\\x1f\\xf1\\xdb\\xfb\\xd5\\xfb&\\xfc\\x93\\xc2\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\", b'\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\nBad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\nBad pipe message: %s [b'!=O\\xbe\\xf6\\x12\\x0e\\x1ah\\xf4R\\x1b@\\xb2\\x88A\\xb4\\x10 \\x83\\xbb^E\\\\0i\\x99\\x96\\x8c\\\\|g\\x01\\x10\\xa7\\xd8\\x15\\xf9E:.3\\xbb\\xc2F\\n<\\n\\t<\\xf4\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1']\nBad pipe message: %s [b\"e\\xb4\\xba\\xac{C\\xb2If\\xdaC\\xd9\\xca\\xab\\x90JD7\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x00\"]\nBad pipe message: %s [b'\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba']\nBad pipe message: %s [b'\"*\\x80?\\xfe\\x91\\x0e\\x1a\\xcc\\x15\\xf5uZ\\x91\\x1d\\xb6~(\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00']\nBad pipe message: %s [b'\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01']\nBad pipe message: %s [b'', b'\\x00\\x02']\nBad pipe message: %s [b'\\x99\\x02*\\xc3\\xd5I\\x85.1\\x80\\xc1\\xd6\\xff%U\\xc9\\x9c\\xcb\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00']\nBad pipe message: %s [b'C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f']\nBad pipe message: %s [b'\\xb8\\x82\\x99&\\x13\\x83uz\\x9a\\xdc\\\\\\xcePo2$\\xf8\\xcf\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0', b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00']\nBad pipe message: %s [b'\\xe3\\x8c\\x84\\x1e\\xb5E\\xde\\xd4\\x83\\x10\\x9dh\\xedy)L\\xcf\\xf0\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005']\nBad pipe message: %s [b'Y\\xfc\\x96o\\xb7^\\xe3\\x0c\\xee,\\x81]\\x16\\x9ex\\x80\\xfdO\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006']"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqnqogd1cV8B",
        "outputId": "5d32b63d-9c31-42ae-a124-b4753ade4a62",
        "gather": {
          "logged": 1670256454665
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot"
      ],
      "metadata": {
        "id": "aoRFdJH1u4CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [idx + 1 for idx in range(len(scores))]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, scores)\n",
        "plt.title('Scores [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(scorefigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "NXe3NShru4CD",
        "gather": {
          "logged": 1670256454933
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_reward = np.cumsum(scores)\n",
        "c_indices = np.arange(len(c_reward))\n",
        "rolling_avg = c_reward / c_indices\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, rolling_avg)\n",
        "plt.title('Average reward [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(cumfigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "y4tPHG0saO2D",
        "gather": {
          "logged": 1670256454955
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(np.arange(len(distances)), distances)\n",
        "plt.title('Distances [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(distfigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hs-oS5UrE8sQ",
        "gather": {
          "logged": 1670256454977
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_distances = np.cumsum(distances)\n",
        "c_indices_distances = np.arange(len(c_distances))\n",
        "rolling_avg_distances = c_distances / c_indices_distances\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(x, rolling_avg_distances)\n",
        "plt.title('Mean distance [' + env_name + ']')\n",
        "plt.grid()\n",
        "save_fig(meandistfigname)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "a3NPuueUFRPI",
        "gather": {
          "logged": 1670256455003
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log"
      ],
      "metadata": {
        "id": "aHnAgjMGu4CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfname = datename + '_runlogs'\n",
        "df_avg_path = 'logs/' + env_name_clean + '_' + dfname + '_avg.csv'\n",
        "df_dist_path = 'logs/' + env_name_clean + '_' + dfname + '_distances.csv'\n",
        "\n",
        "df_avg = pd.DataFrame({'avg_score': rolling_avg})\n",
        "df_dist = pd.DataFrame({'distances': distances})\n",
        "\n",
        "df_avg.to_csv(df_avg_path, sep = ';', header = True, index = False)\n",
        "df_dist.to_csv(df_dist_path, sep = ';', header = True, index = False)\n",
        "\n",
        "print(\"Done logging results\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UU1BATVPu4CF",
        "gather": {
          "logged": 1670256455025
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "76pGlcaiu4CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compscores = 0\n",
        "for i in range(n_rounds):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        env.render()\n",
        "        \n",
        "        action = policy.select_action(state)\n",
        "        \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        replay_buffer.add((state, action, reward, next_state, float(done)))\n",
        "        \n",
        "        state = next_state\n",
        "        score += reward\n",
        "    \n",
        "    compscores += score\n",
        "    print(\"Competitive round \", i + 1, \" Overall score \", compscores)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vjCoTRAtu4CI",
        "gather": {
          "logged": 1670256455047
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score"
      ],
      "metadata": {
        "id": "Lzg3fZnYu4CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(outfilename, \"w\") as f:\n",
        "    f.writelines(\"%s: %i\\n\" % (env_name_clean, compscores))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kdr_4ZppOJs3",
        "gather": {
          "logged": 1670256455070
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(mode == 'colab'):\n",
        "    from google.colab import files\n",
        "    files.download(model_file)\n",
        "    files.download(model_file_target)\n",
        "    files.download(outfilename)\n",
        "    files.download(df_avg_path)\n",
        "    files.download(df_dist_path)\n",
        "    files.download('images/' + scorefigname)\n",
        "    files.download('images/' + cumfigname)\n",
        "    files.download('images/' + distfigname)\n",
        "    files.download('images/' + meandistfigname)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SqaiDFmKOJWg",
        "gather": {
          "logged": 1670256455099
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}