{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only needed in Colab\n",
    "!pip install snntorch\n",
    "!pip install torch\n",
    "!pip install gym\n",
    "!pip install gym[box2d]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from SNNreinforcement import Agent\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "n_games = 500\n",
    "agent = Agent(gamma=0.99, epsilon=0.1, lr=lr, input_dims=[8], n_actions=4, \n",
    "              mem_size=1000000, batch_size=64, epsilon_end=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"lunarlander.png\"\n",
    "scores = [] # Logged every episode\n",
    "eps_history = [] # Logged every episode\n",
    "avg_scores = [] # Logged every 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "agent.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = agent.q_net.state_dict()\n",
    "state[\"lif1.beta\"] = torch.tensor(0.95,dtype=torch.float)\n",
    "state[\"lif2.beta\"] = torch.tensor(0.95, dtype=torch.float)\n",
    "state[\"lif3.beta\"] = torch.tensor(1.0, dtype=torch.float)\n",
    "agent.q_net.load_state_dict(state)\n",
    "print(agent.q_net.lif1.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "n_hrs = 1\n",
    "while time.time() - stime < int(60 * 60 * n_hrs):\n",
    "    i += 1\n",
    "    done = False\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        avg_score = np.mean(scores[max(0, i - 10):(i + 1)])\n",
    "        avg_scores.append(avg_score)\n",
    "        print('episode', i, 'score', score, 'average_score %.3f' % avg_score, 'epsilon %.3f' % agent.epsilon)\n",
    "        agent.save_models()\n",
    "    else:\n",
    "        print('episode', i, 'score', score)\n",
    "    observation, _ = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        if i % 5 == 0:\n",
    "            env.render()\n",
    "        \n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, int(done))\n",
    "        observation = observation_\n",
    "        agent.learn()\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total time: \", time.time() - stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [idx + 1 for idx in range(i + 1)]\n",
    "plt.figure(0)\n",
    "plt.plot(x, scores)\n",
    "plt.grid()\n",
    "plt.figure(1)\n",
    "plt.plot(x, eps_history)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfname = str(datetime.now()).split('.')[0].replace(' ', '_').replace('-', '_').replace(':', '_') + '_runlogs'\n",
    "df = pd.DataFrame({'score': scores, 'Epsilon': eps_history})\n",
    "df_avg = pd.DataFrame({'avg_score': avg_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(dfname+'.csv', sep=';', header=True, index=False)\n",
    "df_avg.to_csv(dfname+'_avg.csv', sep=';', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done logging results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"Press Enter to start trials\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compscores = 0\n",
    "agent.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('epsilon', agent.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    done = False\n",
    "    observation, _ = env.reset()\n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info, _ = env.step(action)\n",
    "        agent.store_transition(observation, action, reward, observation_, int(done))\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "    compscores += score\n",
    "    print(\"Competitive round \", i + 1, \" Overall score \", compscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scoreboard.txt\", \"w\") as f:\n",
    "    f.writelines(\"%s: %i\\n\" % (\"MIkro Lander\", compscores))\n",
    "input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
